{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "dataPath = \"../data/\"\n",
    "newsSample = pd.read_csv(dataPath + \"news_sample.csv\")\n",
    "nsdf = pd.DataFrame(newsSample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0   id                domain        type  \\\n",
      "0           0  141               awm.com  unreliable   \n",
      "1           1  256     beforeitsnews.com        fake   \n",
      "2           2  700           cnnnext.com  unreliable   \n",
      "3           3  768               awm.com  unreliable   \n",
      "4           4  791  bipartisanreport.com   clickbait   \n",
      "\n",
      "                                                 url  \\\n",
      "0  http://awm.com/church-congregation-brings-gift...   \n",
      "1  http://beforeitsnews.com/awakening-start-here/...   \n",
      "2  http://www.cnnnext.com/video/18526/never-hike-...   \n",
      "3  http://awm.com/elusive-alien-of-the-sea-caught...   \n",
      "4  http://bipartisanreport.com/2018/01/21/trumps-...   \n",
      "\n",
      "                                             content  \\\n",
      "0  Sometimes the power of Christmas will make you...   \n",
      "1  AWAKENING OF 12 STRANDS of DNA – “Reconnecting...   \n",
      "2  Never Hike Alone: A Friday the 13th Fan Film U...   \n",
      "3  When a rare shark was caught, scientists were ...   \n",
      "4  Donald Trump has the unnerving ability to abil...   \n",
      "\n",
      "                   scraped_at                 inserted_at  \\\n",
      "0  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "1  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "2  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "3  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "4  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "\n",
      "                   updated_at  \\\n",
      "0  2018-02-02 01:19:41.756664   \n",
      "1  2018-02-02 01:19:41.756664   \n",
      "2  2018-02-02 01:19:41.756664   \n",
      "3  2018-02-02 01:19:41.756664   \n",
      "4  2018-02-02 01:19:41.756664   \n",
      "\n",
      "                                               title          authors  \\\n",
      "0  Church Congregation Brings Gift to Waitresses ...      Ruth Harris   \n",
      "1  AWAKENING OF 12 STRANDS of DNA – “Reconnecting...     Zurich Times   \n",
      "2  Never Hike Alone - A Friday the 13th Fan Film ...              NaN   \n",
      "3  Elusive ‘Alien Of The Sea ‘ Caught By Scientis...  Alexander Smith   \n",
      "4  Trump’s Genius Poll Is Complete & The Results ...  Gloria Christie   \n",
      "\n",
      "   keywords meta_keywords                                   meta_description  \\\n",
      "0       NaN          ['']                                                NaN   \n",
      "1       NaN          ['']                                                NaN   \n",
      "2       NaN          ['']  Never Hike Alone: A Friday the 13th Fan Film  ...   \n",
      "3       NaN          ['']                                                NaN   \n",
      "4       NaN          ['']                                                NaN   \n",
      "\n",
      "  tags  summary  \n",
      "0  NaN      NaN  \n",
      "1  NaN      NaN  \n",
      "2  NaN      NaN  \n",
      "3  NaN      NaN  \n",
      "4  NaN      NaN  \n"
     ]
    }
   ],
   "source": [
    "print(newsSample.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unreliable' 'fake' 'clickbait' 'conspiracy' 'reliable' 'bias' 'hate'\n",
      " 'junksci' 'political' nan 'unknown']\n"
     ]
    }
   ],
   "source": [
    "# unique lable values\n",
    "unique_values = nsdf['type'].unique()\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unreliable' 'fake' 'clickbait' 'conspiracy' 'reliable' 'bias' 'hate'\n",
      " 'junksci' 'political']\n"
     ]
    }
   ],
   "source": [
    "#nan and unknown removed as they seem useless when training a classifier\n",
    "nsdf = nsdf.dropna(subset=['type'])\n",
    "nsdf = nsdf.loc[nsdf['type']!='unknown']\n",
    "newunique_values = nsdf['type'].unique()\n",
    "print(newunique_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords and compute the size of the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "\n",
    "fakeNewsCorpus = pd.read_csv(dataPath + \"995,000_rows.csv\")\n",
    "#Hva saten er den der unnamed???\n",
    "print(fakeNewsCorpus.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 995000 entries, 0 to 994999\n",
      "Data columns (total 17 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   Unnamed: 0        994999 non-null  object \n",
      " 1   id                994993 non-null  object \n",
      " 2   domain            994989 non-null  object \n",
      " 3   type              947214 non-null  object \n",
      " 4   url               994989 non-null  object \n",
      " 5   content           994988 non-null  object \n",
      " 6   scraped_at        994987 non-null  object \n",
      " 7   inserted_at       994987 non-null  object \n",
      " 8   updated_at        994987 non-null  object \n",
      " 9   title             986394 non-null  object \n",
      " 10  authors           552243 non-null  object \n",
      " 11  keywords          0 non-null       float64\n",
      " 12  meta_keywords     956210 non-null  object \n",
      " 13  meta_description  469894 non-null  object \n",
      " 14  tags              230919 non-null  object \n",
      " 15  summary           0 non-null       float64\n",
      " 16  source            214922 non-null  object \n",
      "dtypes: float64(2), object(15)\n",
      "memory usage: 129.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(fakeNewsCorpus.info())   # Check column types and missing values\n",
    "fndf = pd.DataFrame(fakeNewsCorpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                                                        732\n",
       "id                                                          7444726.0\n",
       "domain                                             nationalreview.com\n",
       "type                                                        political\n",
       "url                 http://www.nationalreview.com/node/152734/%E2%...\n",
       "content             Plus one article on Google Plus\\n\\n(Thanks to ...\n",
       "scraped_at                                 2017-11-27T01:14:42.983556\n",
       "inserted_at                                2018-02-08 19:18:34.468038\n",
       "updated_at                                 2018-02-08 19:18:34.468066\n",
       "title                                              Iran News Round Up\n",
       "authors                                                           NaN\n",
       "keywords                                                          NaN\n",
       "meta_keywords       ['National Review', 'National Review Online', ...\n",
       "meta_description                                                  NaN\n",
       "tags                                                              NaN\n",
       "summary                                                           NaN\n",
       "source                                                            NaN\n",
       "Name: 0, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Pandas DataFrame:\")\n",
    "display(fndf.iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['political' 'fake' 'satire' 'reliable' 'conspiracy' 'unreliable' 'bias'\n",
      " 'rumor' 'unknown' nan 'clickbait' 'hate' 'junksci'\n",
      " '2018-02-10 13:43:39.521661']\n"
     ]
    }
   ],
   "source": [
    "# unique lable values\n",
    "unique_values = fndf['type'].unique()\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['political' 'fake' 'satire' 'reliable' 'conspiracy' 'unreliable' 'bias'\n",
      " 'rumor' 'clickbait' 'hate' 'junksci' '2018-02-10 13:43:39.521661']\n"
     ]
    }
   ],
   "source": [
    "#hard to know how to classify nan and unknown, so removed for now\n",
    "fndf = fndf.dropna(subset=['type'])\n",
    "fndf = fndf.loc[fndf['type']!='unknown']\n",
    "\n",
    "newunique_values = fndf['type'].unique()\n",
    "print(newunique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '1']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "903680"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groups (reliable) as truenews 1 and (all others) in fakenews 0 \n",
    "    #note this is naive and should be reconsidered later\n",
    "fndf['type'] = fndf['type'].replace(r'^reliable$', '1', regex=True)  # Only replaces exact 'reliable' with 1\n",
    "fndf['type'] = fndf['type'].replace(r'^(?!1$).+', '0', regex=True)   # Replace everything except '1' with '0'\n",
    "#fndf['type'] = fndf['type'].fillna('0')\n",
    "\n",
    "newunique_values = fndf['type'].unique()\n",
    "print(newunique_values)\n",
    "fndf.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: (722944,)\n",
      "val size: (90368,)\n",
      "test size: (90368,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting into test, train and validation\n",
    "X_train, X_valtest, y_train, y_valtest = train_test_split(fndf['content'], fndf['type'], test_size=0.2, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_valtest, y_valtest, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"train size:\", y_train.shape)\n",
    "print(\"val size:\", y_val.shape)\n",
    "print(\"test size:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
