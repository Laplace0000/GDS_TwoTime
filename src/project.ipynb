{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n",
      "[nltk_data] Downloading package punkt to /home/andreas-linus-thalund-\n",
      "[nltk_data]     midtgaard/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/andreas-linus-\n",
      "[nltk_data]     thalund-midtgaard/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.lm import Vocabulary\n",
    "from sklearn.model_selection import train_test_split\n",
    "from cleantext import clean\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords') \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 250 entries, 0 to 249\n",
      "Data columns (total 16 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Unnamed: 0        250 non-null    int64  \n",
      " 1   id                250 non-null    int64  \n",
      " 2   domain            250 non-null    object \n",
      " 3   type              238 non-null    object \n",
      " 4   url               250 non-null    object \n",
      " 5   content           250 non-null    object \n",
      " 6   scraped_at        250 non-null    object \n",
      " 7   inserted_at       250 non-null    object \n",
      " 8   updated_at        250 non-null    object \n",
      " 9   title             250 non-null    object \n",
      " 10  authors           170 non-null    object \n",
      " 11  keywords          0 non-null      float64\n",
      " 12  meta_keywords     250 non-null    object \n",
      " 13  meta_description  54 non-null     object \n",
      " 14  tags              27 non-null     object \n",
      " 15  summary           0 non-null      float64\n",
      "dtypes: float64(2), int64(2), object(12)\n",
      "memory usage: 31.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "dataPath = \"../data/\"\n",
    "newsSample = pd.read_csv(dataPath + \"news_sample.csv\")\n",
    "nsdf = pd.DataFrame(newsSample)\n",
    "nsdf = nsdf.reset_index(drop=True)  # Reset index??\n",
    "nsdf_raw = nsdf\n",
    "print(nsdf.info())   # Check column types and missing values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unreliable' 'fake' 'clickbait' 'conspiracy' 'reliable' 'bias' 'hate'\n",
      " 'junksci' 'political' nan 'unknown']\n"
     ]
    }
   ],
   "source": [
    "# unique lable values\n",
    "unique_values = nsdf['type'].unique()\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unreliable' 'fake' 'clickbait' 'conspiracy' 'reliable' 'bias' 'hate'\n",
      " 'junksci' 'political']\n"
     ]
    }
   ],
   "source": [
    "#nan and unknown removed as they seem useless when training a classifier\n",
    "nsdf = nsdf.dropna(subset=['type'])\n",
    "nsdf = nsdf.loc[nsdf['type']!='unknown']\n",
    "newunique_values = nsdf['type'].unique()\n",
    "print(newunique_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(data, column):\n",
    "    def clean_text_help(text):\n",
    "        if isinstance(text, str):\n",
    "            # Remove excess whitespace\n",
    "            text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "            #replace dates\n",
    "            text = re.sub(r\"(0[1-9]|[1-2][0-9]|3[0-1])[-/.]?(0[1-9]|1[0-2])[-/.]?([0-9]{2}|[0-9]{4})\", \"<DATE>\", text)  # Replace date type 1\n",
    "            text = re.sub(r\"(0[1-9]|[1-2][0-9]|3[0-1])\\s([A-Za-z]{3})\\s([0-9]{2}|[0-9]{4})\", \"<DATE>\", text)  # Replace date type 2\n",
    "            return clean(text, lower=True, no_line_breaks=True, replace_with_url=\"<URL>\", replace_with_email=\"<EMAIL>\", replace_with_number=\"<NUM>\", replace_with_digit=\"<NUM>\")\n",
    "        return text  # Return unchanged if not a string\n",
    "    data[column] = data[column].apply(clean_text_help)  # Apply function\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the text function\n",
    "def tokenizeText(data, column):\n",
    "    def tokenize_text_help(text):\n",
    "        if isinstance(text, str):\n",
    "            return word_tokenize(text)\n",
    "        return text  # Return unchanged if not a string\n",
    "    data[column] = data[column].apply(tokenize_text_help)  # Apply function\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for removeing stopwords\n",
    "def remove_stopwords(data, column):\n",
    "    def remove_stopwords_help(text):\n",
    "        stop_words = set(stopwords.words('english'))  # Load stopwords\n",
    "        if isinstance(text, str):\n",
    "            return [word for word in text.at[0, 'content'] if not word.lower() in stop_words]\n",
    "        return text  # Return unchanged if not a string\n",
    "    data[column] = data[column].apply(remove_stopwords_help)  # Apply function\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funtion for populating vocabulary\n",
    "def populate_vocabulary(data):\n",
    "    N = data.shape[0]  # Get the number of rows\n",
    "    allWords = []\n",
    "    for i in range(N):\n",
    "        if isinstance(data.at[i, 'content'], str):  # Ensure it's a string\n",
    "            allWords.append(data.at[i, 'content'])\n",
    "    return Vocabulary(allWords, unk_cutoff=2)\n",
    "\n",
    "#langsom kørertid men kunne ikke finde ud af det med apply. Nogne med en god ide??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making frequency dictionary\n",
    "def getFrequency(data, column, preTokenized):\n",
    "    N = data.shape[0]  # Get the number of rows\n",
    "    allWords = []\n",
    "    if preTokenized != 1 and preTokenized != 0:\n",
    "        return \"Wrong preTokenize input\"\n",
    "    if preTokenized == 1:\n",
    "        for text in data[column]:\n",
    "            if isinstance(text, list):  # Ensure text is already tokenized (list of words)\n",
    "                allWords.extend(text)\n",
    "            elif isinstance(text, str):  # If still a string, split it as a fallback\n",
    "                allWords.extend(text.split())\n",
    "    elif preTokenized == 0:\n",
    "        for text in data[column]:\n",
    "            if isinstance(text, str):  \n",
    "                allWords.extend(word_tokenize(text))\n",
    "\n",
    "    return FreqDist(allWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for removeing stopwords\n",
    "def dataStemming(data, column):\n",
    "    ps = PorterStemmer()\n",
    "    def dataStemming_help(text):\n",
    "        if isinstance(text, str):\n",
    "            return ps.stem(text)\n",
    "        return text  # Return unchanged if not a string\n",
    "    data[column] = data[column].apply(dataStemming_help)  # Apply function\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sometimes', 'the', 'power', 'of', 'christmas', 'will', 'make', 'you', 'do', 'wild', 'and', 'wonderful', 'things', '.', 'you', 'do', 'not', 'need', 'to', 'believe', 'in', 'the', 'holy', 'trinity', 'to', 'believe', 'in', 'the', 'positive', 'power', 'of', 'doing', 'good', 'for', 'others', '.', 'the', 'simple', 'act', 'of', 'giving', 'without', 'receiving', 'is', 'lost', 'on', 'many', 'of', 'us', 'these', 'days', ',', 'as', 'worries', 'about', 'money', 'and', 'success', 'hold', 'us', 'back', 'from', 'giving', 'to', 'others', 'who', 'are', 'in', 'need', '.', 'one', 'congregation', 'in', 'ohio', 'was', 'moved', 'to', 'action', 'by', 'the', 'power', 'of', 'a', 'sermon', 'given', 'at', 'their', 'church', 'on', 'christmas', 'eve', '.', 'the', 'pastor', 'at', 'grand', 'lake', 'united', 'methodist', 'church', 'in', 'celina', ',', 'ohio', 'gave', 'an', 'emotional', 'sermon', 'about', 'the', 'importance', 'of', 'understanding', 'the', 'message', 'of', 'jesus', '.', 'for', 'many', 'religious', 'people', 'the', 'message', 'of', 'jesus', 'is', 'to', 'help', 'others', 'before', 'yourself', ',', 'to', 'make', 'sure', 'the', 'people', 'who', 'are', 'suffering', 'get', 'the', 'help', 'they', 'need', 'to', 'enjoy', 'life', 'a', 'little', 'bit', '.', 'the', 'sermon', 'was', 'really', 'about', 'generosity', 'and', 'what', 'that', 'can', 'look', 'like', 'in', 'our', 'lives', '.', 'jesus', 'lived', 'a', 'long', 'time', 'ago', 'and', 'he', 'acted', 'generously', 'in', 'the', 'fashion', 'of', 'his', 'time', 'but', 'what', 'would', 'a', 'generous', 'act', 'look', 'like', 'in', 'our', 'times', '?', 'that', 'was', 'the', 'focus', 'of', 'the', 'sermon', '.', 'the', 'potency', 'of', 'the', 'sermon', 'was', 'not', 'lost', 'on', 'the', 'congregation', ',', 'who', 'were', 'so', 'moved', 'they', 'had', 'to', 'take', 'action', '!', 'after', 'the', 'sermon', 'ended', ',', 'the', 'congregation', 'decided', 'to', 'take', 'an', 'offering', '.', 'a', 'bowl', 'was', 'passed', 'around', 'the', 'room', 'and', 'everyone', 'pitched', 'in', 'what', 'they', 'could', 'on', 'this', 'christmas', 'eve', 'with', 'the', 'words', 'of', 'the', 'sermon', 'still', 'ringing', 'in', 'their', 'ears', '.', 'what', 'did', 'they', 'do', 'with', 'this', 'offering', '?', 'members', 'of', 'the', 'congregation', 'drove', 'down', 'to', 'the', 'local', 'waffle', 'house', 'to', 'visit', 'the', 'ladies', 'working', 'the', 'night', 'shift', '.', 'what', 'a', 'great', 'choice', 'on', 'this', 'most', 'holy', 'of', 'days', 'when', 'everyone', 'should', 'be', 'with', 'their', 'families', '!', 'the', 'ladies', 'working', 'at', 'waffle', 'house', 'clearly', 'were', 'not', 'with', 'their', 'families', '.', 'they', 'had', 'no', 'choice', 'but', 'to', 'work', 'on', 'this', 'holy', 'day', 'because', 'it', 'paid', 'the', 'bills', '.', 'the', 'congregation', 'understood', 'the', 'sacrifice', 'being', 'made', 'by', 'these', 'ladies', ',', 'and', 'wanted', 'to', 'help', 'them', 'out', '.', 'they', 'donated', 'the', 'entire', 'offering', 'to', 'be', 'split', 'amongst', 'the', 'ladies', 'at', 'waffle', 'house', '.', 'in', 'total', 'that', 'amounted', 'to', '$', '3,500', 'being', 'split', 'amongst', 'the', 'staff', '.', 'what', 'a', 'beautiful', 'moment', '!', 'what', 'a', 'perfect', 'example', 'of', 'what', 'the', 'preacher', 'was', 'talking', 'about', 'in', 'his', 'sermon', '!', 'doing', 'a', 'good', 'deed', 'like', 'this', 'on', 'christmas', 'really', 'helped', 'ease', 'the', 'burden', 'felt', 'by', 'the', 'ladies', 'working', 'at', 'waffle', 'house', '.', 'sure', ',', 'they', 'could', 'not', 'see', 'their', 'families', ',', 'but', 'at', 'least', 'they', 'got', 'a', 'little', 'gift', 'from', 'the', 'good', 'people', 'of', 'their', 'community', '.', 'perhaps', 'the', 'best', 'part', 'about', 'this', 'whole', 'event', 'was', 'that', 'the', 'congregation', 'did', 'not', 'ask', 'anything', 'in', 'return', '.', 'it', 'was', 'a', 'simple', 'act', 'of', 'generosity', 'from', 'people', 'who', 'understood', 'the', 'pain', 'being', 'felt', 'by', 'another', 'group', 'and', 'sought', 'to', 'alleviate', 'some', 'of', 'that', 'pain', '.', 'it', 'speaks', 'volumes', 'about', 'the', 'merits', 'of', 'the', 'church', 'in', 'our', 'daily', 'lives', '.', 'this', 'simple', 'act', 'brought', 'the', 'entire', 'community', 'together', 'because', 'it', 'showed', 'empathy', 'and', 'compassion', 'on', 'the', 'most', 'special', 'day', 'of', 'the', 'year', '.']\n"
     ]
    }
   ],
   "source": [
    "nsdf_cleaned = cleanText(nsdf, 'content')\n",
    "nsdf_tokenized = tokenizeText(nsdf_cleaned, 'content')                  #tokenizing\n",
    "nsdf_cleaned_tokenized_nostopwords = remove_stopwords(nsdf_tokenized, 'content')           #removing stopwords\n",
    "nsdf_preprocessed = dataStemming(nsdf_cleaned_tokenized_nostopwords, 'content')\n",
    "print(nsdf_preprocessed.at[0, 'content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word frequency pre preprocessing\n",
      "198366\n"
     ]
    }
   ],
   "source": [
    "#word frequency pre preprocessing\n",
    "print(\"word frequency pre preprocessing\")\n",
    "word_frequency_pre = getFrequency(nsdf_raw, 'content', 0)\n",
    "print(sum(word_frequency_pre.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word frequency post preprocessing\n",
      "173731\n"
     ]
    }
   ],
   "source": [
    "#word frequency post preprocessing\n",
    "print(\"word frequency post preprocessing\")\n",
    "word_frequency_post = getFrequency(nsdf_cleaned_tokenized_nostopwords, 'content', 1)\n",
    "print(sum(word_frequency_post.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word frequency post stemming\n",
      "173731\n"
     ]
    }
   ],
   "source": [
    "#word frequency post stemming\n",
    "print(\"word frequency post stemming\")\n",
    "word_frequency_postStem = getFrequency(nsdf_preprocessed, 'content', 1)\n",
    "print(sum(word_frequency_postStem.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction rate of the vocabulary size after removing stopwords:\n",
      "24635\n",
      "Further  reduction rate of the vocabulary size after stemming\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "pre = sum(word_frequency_pre.values())\n",
    "post = sum(word_frequency_post.values())\n",
    "print(\"Reduction rate of the vocabulary size after removing stopwords:\")\n",
    "print(abs(pre -post))\n",
    "print(\"Further  reduction rate of the vocabulary size after stemming\")\n",
    "postStem = sum(word_frequency_postStem.values())\n",
    "print(abs(post -postStem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65266/2657420253.py:3: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  fakeNewsCorpus = pd.read_csv(dataPath + \"995,000_rows.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Unnamed: 0         id               domain        type  \\\n",
      "0        732  7444726.0   nationalreview.com   political   \n",
      "1       1348  6213642.0    beforeitsnews.com        fake   \n",
      "2       7119  3867639.0     dailycurrant.com      satire   \n",
      "3       1518  9560791.0          nytimes.com    reliable   \n",
      "4       9345  2059625.0  infiniteunknown.net  conspiracy   \n",
      "\n",
      "                                                 url  \\\n",
      "0  http://www.nationalreview.com/node/152734/%E2%...   \n",
      "1  http://beforeitsnews.com/economy/2012/06/the-c...   \n",
      "2  http://dailycurrant.com/2016/01/18/man-awoken-...   \n",
      "3  https://query.nytimes.com/gst/fullpage.html?re...   \n",
      "4  http://www.infiniteunknown.net/2011/09/14/100-...   \n",
      "\n",
      "                                             content  \\\n",
      "0  Plus one article on Google Plus\\n\\n(Thanks to ...   \n",
      "1  The Cost Of The Best Senate Banking Committee ...   \n",
      "2  Man Awoken From 27-Year Coma Commits Suicide A...   \n",
      "3  WHEN Julia Geist was asked to draw a picture o...   \n",
      "4  – 100 Compiled Studies on Vaccine Dangers (Act...   \n",
      "\n",
      "                   scraped_at                 inserted_at  \\\n",
      "0  2017-11-27T01:14:42.983556  2018-02-08 19:18:34.468038   \n",
      "1    2017-11-27T01:14:08.7454  2018-02-08 19:18:34.468038   \n",
      "2  2017-11-27T01:14:21.395055  2018-02-07 23:39:33.852671   \n",
      "3  2018-02-11 00:46:42.632962  2018-02-11 00:14:20.346838   \n",
      "4  2017-11-10T11:18:44.524042  2018-02-07 23:39:33.852671   \n",
      "\n",
      "                   updated_at  \\\n",
      "0  2018-02-08 19:18:34.468066   \n",
      "1  2018-02-08 19:18:34.468066   \n",
      "2  2018-02-07 23:39:33.852696   \n",
      "3  2018-02-11 00:14:20.346871   \n",
      "4  2018-02-07 23:39:33.852696   \n",
      "\n",
      "                                               title authors  keywords  \\\n",
      "0                                 Iran News Round Up     NaN       NaN   \n",
      "1  The Cost Of The Best Senate Banking Committee ...     NaN       NaN   \n",
      "2  Man Awoken From 27-Year Coma Commits Suicide A...     NaN       NaN   \n",
      "3  Opening a Gateway for Girls to Enter the Compu...     NaN       NaN   \n",
      "4  100 Compiled Studies on Vaccine Dangers – Infi...     NaN       NaN   \n",
      "\n",
      "                                       meta_keywords  \\\n",
      "0  ['National Review', 'National Review Online', ...   \n",
      "1                                               ['']   \n",
      "2                                               ['']   \n",
      "3  ['Computers and the Internet', 'Women and Girl...   \n",
      "4                                               ['']   \n",
      "\n",
      "                                    meta_description  \\\n",
      "0                                                NaN   \n",
      "1                                                NaN   \n",
      "2                                                NaN   \n",
      "3  WHEN Julia Geist was asked to draw a picture o...   \n",
      "4                                                NaN   \n",
      "\n",
      "                                                tags  summary   source  \n",
      "0                                                NaN      NaN      NaN  \n",
      "1                                                NaN      NaN      NaN  \n",
      "2                                                NaN      NaN      NaN  \n",
      "3                                                NaN      NaN  nytimes  \n",
      "4  Lymphoma, Hepatitis B, Immune System, Health, ...      NaN      NaN  \n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "\n",
    "fakeNewsCorpus = pd.read_csv(dataPath + \"995,000_rows.csv\")\n",
    "#Hva saten er den der unnamed???\n",
    "print(fakeNewsCorpus.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 995000 entries, 0 to 994999\n",
      "Data columns (total 17 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   Unnamed: 0        994999 non-null  object \n",
      " 1   id                994993 non-null  object \n",
      " 2   domain            994989 non-null  object \n",
      " 3   type              947214 non-null  object \n",
      " 4   url               994989 non-null  object \n",
      " 5   content           994988 non-null  object \n",
      " 6   scraped_at        994987 non-null  object \n",
      " 7   inserted_at       994987 non-null  object \n",
      " 8   updated_at        994987 non-null  object \n",
      " 9   title             986394 non-null  object \n",
      " 10  authors           552243 non-null  object \n",
      " 11  keywords          0 non-null       float64\n",
      " 12  meta_keywords     956210 non-null  object \n",
      " 13  meta_description  469894 non-null  object \n",
      " 14  tags              230919 non-null  object \n",
      " 15  summary           0 non-null       float64\n",
      " 16  source            214922 non-null  object \n",
      "dtypes: float64(2), object(15)\n",
      "memory usage: 129.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(fakeNewsCorpus.info())   # Check column types and missing values\n",
    "fndf = pd.DataFrame(fakeNewsCorpus)\n",
    "fndf = fndf.reset_index(drop=True)  # Reset index??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, id, domain, type, url, content, scraped_at, inserted_at, updated_at, title, authors, keywords, meta_keywords, meta_description, tags, summary, source]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(fndf.loc[fndf['type']==''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                                                        732\n",
       "id                                                          7444726.0\n",
       "domain                                             nationalreview.com\n",
       "type                                                        political\n",
       "url                 http://www.nationalreview.com/node/152734/%E2%...\n",
       "content             Plus one article on Google Plus\\n\\n(Thanks to ...\n",
       "scraped_at                                 2017-11-27T01:14:42.983556\n",
       "inserted_at                                2018-02-08 19:18:34.468038\n",
       "updated_at                                 2018-02-08 19:18:34.468066\n",
       "title                                              Iran News Round Up\n",
       "authors                                                           NaN\n",
       "keywords                                                          NaN\n",
       "meta_keywords       ['National Review', 'National Review Online', ...\n",
       "meta_description                                                  NaN\n",
       "tags                                                              NaN\n",
       "summary                                                           NaN\n",
       "source                                                            NaN\n",
       "Name: 0, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Pandas DataFrame:\")\n",
    "display(fndf.iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['political' 'fake' 'satire' 'reliable' 'conspiracy' 'unreliable' 'bias'\n",
      " 'rumor' 'unknown' nan 'clickbait' 'hate' 'junksci'\n",
      " '2018-02-10 13:43:39.521661']\n"
     ]
    }
   ],
   "source": [
    "# unique lable values\n",
    "unique_values = fndf['type'].unique()\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['political' 'fake' 'satire' 'reliable' 'conspiracy' 'unreliable' 'bias'\n",
      " 'rumor' 'clickbait' 'hate' 'junksci' '2018-02-10 13:43:39.521661']\n"
     ]
    }
   ],
   "source": [
    "#hard to know how to classify nan and unknown, so removed for now\n",
    "fndf = fndf.dropna(subset=['type'])\n",
    "fndf = fndf.loc[fndf['type']!='unknown']\n",
    "\n",
    "newunique_values = fndf['type'].unique()\n",
    "print(newunique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '1']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "903680"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groups (reliable) as truenews 1 and (all others) in fakenews 0 \n",
    "    #note this is naive and should be reconsidered later\n",
    "fndf['type'] = fndf['type'].replace(r'^reliable$', '1', regex=True)  # Only replaces exact 'reliable' with 1\n",
    "fndf['type'] = fndf['type'].replace(r'^(?!1$).+', '0', regex=True)   # Replace everything except '1' with '0'\n",
    "#fndf['type'] = fndf['type'].fillna('0')\n",
    "\n",
    "newunique_values = fndf['type'].unique()\n",
    "print(newunique_values)\n",
    "fndf.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: (722944,)\n",
      "val size: (90368,)\n",
      "test size: (90368,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting into test, train and validation\n",
    "X_train, X_valtest, y_train, y_valtest = train_test_split(fndf['content'], fndf['type'], test_size=0.2, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_valtest, y_valtest, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"train size:\", y_train.shape)\n",
    "print(\"val size:\", y_val.shape)\n",
    "print(\"test size:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
