{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afP0fI0WAjLQ",
        "outputId": "63969ef1-021e-4875-bd33-e77080113061"
      },
      "outputs": [],
      "source": [
        "# prompt: please install all the below using pip\n",
        "\n",
        "#!pip install numpy pandas matplotlib seaborn scikit-learn tensorflow keras clean_text pandarallel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7I5ZeAs7doa",
        "outputId": "f04e908c-53cf-4e5e-b1ac-6c4eeba64133"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO: Pandarallel will run on 8 workers.\n",
            "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/fridakorzen-\n",
            "[nltk_data]     bohr/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /Users/fridakorzen-\n",
            "[nltk_data]     bohr/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /Users/fridakorzen-\n",
            "[nltk_data]     bohr/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.lm import Vocabulary\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.preprocessing import StandardScaler, Normalizer\n",
        "import joblib\n",
        "from cleantext import clean\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.stem import PorterStemmer\n",
        "from pandarallel import pandarallel\n",
        "import ast\n",
        "import math\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "pandarallel.initialize(progress_bar=False)\n",
        "import time\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "import os\n",
        "IN_COLAB = False\n",
        "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
        "   IN_COLAB = True\n",
        "\n",
        "news_processed = None\n",
        "bbc_proccessed = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DWSzbnj75b9",
        "outputId": "79c1884d-37d0-4041-ee74-66a39de326ab"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWp-AaTm7doc"
      },
      "source": [
        "# Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAkg7frj7dod"
      },
      "source": [
        "### Task 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0lGjcG17dod",
        "outputId": "9d148c02-9df3-4945-ad46-60fa24a7a724"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 250 entries, 0 to 249\n",
            "Data columns (total 16 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   Unnamed: 0        250 non-null    int64  \n",
            " 1   id                250 non-null    int64  \n",
            " 2   domain            250 non-null    object \n",
            " 3   type              238 non-null    object \n",
            " 4   url               250 non-null    object \n",
            " 5   content           250 non-null    object \n",
            " 6   scraped_at        250 non-null    object \n",
            " 7   inserted_at       250 non-null    object \n",
            " 8   updated_at        250 non-null    object \n",
            " 9   title             250 non-null    object \n",
            " 10  authors           170 non-null    object \n",
            " 11  keywords          0 non-null      float64\n",
            " 12  meta_keywords     250 non-null    object \n",
            " 13  meta_description  54 non-null     object \n",
            " 14  tags              27 non-null     object \n",
            " 15  summary           0 non-null      float64\n",
            "dtypes: float64(2), int64(2), object(12)\n",
            "memory usage: 31.4+ KB\n"
          ]
        }
      ],
      "source": [
        "#load data\n",
        "dataPath = \"../data/\"\n",
        "if IN_COLAB:\n",
        "  dataPath = \"/content/drive/MyDrive/\"\n",
        "nsdf = pd.read_csv(dataPath + \"news_sample.csv\")\n",
        "nsdf = nsdf.reset_index(drop=True)  # Reset index??\n",
        "nsdf_raw = nsdf.copy(deep=True)\n",
        "nsdf.info()   # Check column types and missing values\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw3NaRAQ7doe",
        "outputId": "40b81a48-d87e-44ce-cc4d-775c58a7663b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['unreliable' 'fake' 'clickbait' 'conspiracy' 'reliable' 'bias' 'hate'\n",
            " 'junksci' 'political' nan 'unknown']\n"
          ]
        }
      ],
      "source": [
        "# unique lable values\n",
        "unique_values = nsdf['type'].unique()\n",
        "print(unique_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Sr01Nsx7doe",
        "outputId": "f841641a-6f2a-4902-e47e-ddd941ae53dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['unreliable' 'fake' 'clickbait' 'conspiracy' 'reliable' 'bias' 'hate'\n",
            " 'junksci' 'political']\n"
          ]
        }
      ],
      "source": [
        "#nan and unknown removed as they seem useless when training a classifier\n",
        "nsdf = nsdf.dropna(subset=['type'])\n",
        "nsdf = nsdf.loc[nsdf['type']!='unknown']\n",
        "newunique_values = nsdf['type'].unique()\n",
        "print(newunique_values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "# try:\n",
        "#     nsdf.drop(columns=['Unnamed: 0', 'keywords', 'summary', 'inserted_at', 'updated_at'], inplace=True)\n",
        "# except:\n",
        "#     pass\n",
        "# nsdf.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bdqH5Za7doe"
      },
      "source": [
        "Cleaning and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "0aqMgikT7doe"
      },
      "outputs": [],
      "source": [
        "def cleanText(data, column):\n",
        "    data[column] = data[column].parallel_apply(clean_text_help)\n",
        "    return data\n",
        "\n",
        "def clean_text_help(text):\n",
        "    if isinstance(text, str):\n",
        "        # Remove excess whitespace\n",
        "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "        #replace dates\n",
        "        text = re.sub(r\"(0[1-9]|[1-2][0-9]|3[0-1])[-/.]?(0[1-9]|1[0-2])[-/.]?([0-9]{2}|[0-9]{4})\", \"<DATE>\", text)  # Replace date type 1\n",
        "        text = re.sub(r\"(0[1-9]|[1-2][0-9]|3[0-1])\\s([A-Za-z]{3})\\s([0-9]{2}|[0-9]{4})\", \"<DATE>\", text)  # Replace date type 2\n",
        "        return clean(text, lower=True, no_line_breaks=True, no_numbers=True, no_emails=True, no_urls=True, no_punct=True, replace_with_url=r\"__URL__\", replace_with_email=r\"__EMAIL__\", replace_with_number=r\"__NUM__\", replace_with_digit=r\"__NUM__\")\n",
        "    raise TypeError(\"Clean_text passed non-string\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "pCu5do7_7doe"
      },
      "outputs": [],
      "source": [
        "#Tokenize the text function\n",
        "def tokenizeText(data, column):\n",
        "    def tokenize_text_help(text):\n",
        "        if isinstance(text, str):\n",
        "            return (word_tokenize(text))\n",
        "        return text  # Return unchanged if not a string\n",
        "    data[column] = data[column].parallel_apply(tokenize_text_help)  # Apply function\n",
        "    return data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "NieKcD1f7dof"
      },
      "outputs": [],
      "source": [
        "#function for removeing stopwords\n",
        "def remove_stopwords_help(text):\n",
        "    text = pd.Series(text)\n",
        "    stop_words = set(stopwords.words('english'))  # Load stopwords\n",
        "    return text[~text.isin(stop_words)].to_list()  # Remove stopwords\n",
        "\n",
        "def remove_stopwords(data, column):\n",
        "    data['content'] = data['content'].parallel_apply(remove_stopwords_help)  # Apply function\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HPQfCwE7dof"
      },
      "outputs": [],
      "source": [
        "#Returns a pandas series, with word and frequency, very fast.\n",
        "def getFreq(data, column):\n",
        "    return len(data[column].str.split().explode().value_counts())\n",
        "\n",
        "def getFreq_tokinized(data, column):\n",
        "    return len(data[column].explode().value_counts())\n",
        "\n",
        "def Vocab_size_tokinized(data, column):\n",
        "    return len(data[column].explode())\n",
        "\n",
        "def Vocab_size(data, column):\n",
        "    return len(data[column].str.split().explode())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "IjoX2dVY7dof"
      },
      "outputs": [],
      "source": [
        "\n",
        "#function for removeing stopwords\n",
        "def dataStemming(data, column):\n",
        "    ps = PorterStemmer()\n",
        "    def dataStemming_help(text):\n",
        "        text = pd.Series(text)\n",
        "        if(isinstance(text, str)):\n",
        "            return pd.Series(ps.stem(text))\n",
        "        return text.apply(ps.stem).to_list()\n",
        "    data[column] = data[column].parallel_apply(dataStemming_help)  # Apply function\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Unique words ---\n",
            "Raw:  30005\n",
            "Preprocessing:  27721\n",
            "After cleaning:  15649\n",
            "After tokenizing:  15641\n",
            "After removing stopwords:  15509\n",
            "After stemming:  10486\n",
            " \n",
            "--- Word counts ---\n",
            "Raw:  170462\n",
            "Preprosseing:  151344\n",
            "After cleaning:  150431\n",
            "After tokenizing:  150679\n",
            "After removing stopwords:  86067\n",
            "After stemming:  86067\n"
          ]
        }
      ],
      "source": [
        "printing_copy = nsdf_raw.copy(deep=True)\n",
        "freq_raw = getFreq(printing_copy, 'content')\n",
        "data_size_raw = Vocab_size(printing_copy, 'content')\n",
        "\n",
        "printing_copy = printing_copy.dropna(subset=['type'])\n",
        "printing_copy = printing_copy.loc[nsdf['type']!='unknown']\n",
        "\n",
        "freq_pre = getFreq(printing_copy, 'content')\n",
        "data_size_pre = Vocab_size(printing_copy, 'content')\n",
        "freq_clean = getFreq(cleanText(printing_copy, 'content'), 'content')\n",
        "data_size_clean = Vocab_size(printing_copy, 'content')\n",
        "freq_token = getFreq_tokinized(tokenizeText(printing_copy, 'content'), 'content')\n",
        "data_size_token = Vocab_size_tokinized(printing_copy, 'content')\n",
        "freq_stopwords = getFreq_tokinized(remove_stopwords(printing_copy, 'content'), 'content')\n",
        "data_size_stopwords = Vocab_size_tokinized(printing_copy, 'content')\n",
        "freq_stemmed = getFreq_tokinized(dataStemming(printing_copy, 'content'), 'content')\n",
        "data_size_stemmed = Vocab_size_tokinized(printing_copy, 'content')\n",
        "\n",
        "\n",
        "print(\"--- Unique words ---\")\n",
        "print(\"Raw: \", freq_raw)\n",
        "print(\"Preprocessing: \", freq_pre)\n",
        "print(\"After cleaning: \", freq_clean)\n",
        "print(\"After tokenizing: \", freq_token)\n",
        "print(\"After removing stopwords: \", freq_stopwords)\n",
        "print(\"After stemming: \", freq_stemmed)\n",
        "\n",
        "print(\" \")\n",
        "\n",
        "print(\"--- Word counts ---\")\n",
        "print(\"Raw: \", data_size_raw)\n",
        "print(\"Preprosseing: \", data_size_pre)\n",
        "print(\"After cleaning: \", data_size_clean)\n",
        "print(\"After tokenizing: \", data_size_token)\n",
        "print(\"After removing stopwords: \", data_size_stopwords)\n",
        "print(\"After stemming: \", data_size_stemmed)\n",
        "\n",
        "\n",
        "#Delete copy:\n",
        "del printing_copy\n",
        "del freq_pre, data_size_pre, freq_clean, data_size_clean, freq_token, data_size_token, freq_stopwords, data_size_stopwords, freq_stemmed, data_size_stemmed\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "MGqT6iu77dof"
      },
      "outputs": [],
      "source": [
        "# One big function to process data:\n",
        "def processData(data, column):\n",
        "    def apply_sequential_helper(functions):\n",
        "        # assume type siganture of functions to be List[f : String -> string ]\n",
        "        def inner(text):\n",
        "            for f in functions:\n",
        "                text = f(text)\n",
        "            return text\n",
        "        return inner\n",
        "\n",
        "    def clean_text_help(text):\n",
        "        if isinstance(text, str):\n",
        "            # Remove excess whitespace\n",
        "            text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "            #replace dates\n",
        "            text = re.sub(r\"(0[1-9]|[1-2][0-9]|3[0-1])[-/.]?(0[1-9]|1[0-2])[-/.]?([0-9]{2}|[0-9]{4})\", \"<DATE>\", text)  # Replace date type 1\n",
        "            text = re.sub(r\"(0[1-9]|[1-2][0-9]|3[0-1])\\s([A-Za-z]{3})\\s([0-9]{2}|[0-9]{4})\", \"<DATE>\", text)  # Replace date type 2\n",
        "            return clean(text, lower=True, no_line_breaks=True, no_numbers=True, no_emails=True, no_urls=True, no_punct=True, replace_with_url=r\"__URL__\", replace_with_email=r\"__EMAIL__\", replace_with_number=r\"__NUM__\", replace_with_digit=r\"__NUM__\")\n",
        "        raise TypeError(\"Clean_text passed non-string\")\n",
        "\n",
        "    def tokenize_text_help(text):\n",
        "        if isinstance(text, str):\n",
        "            return pd.Series(word_tokenize(text))\n",
        "        return text  # Return unchanged if not a string\n",
        "\n",
        "    def remove_stopwords_help(text):\n",
        "      # text is a Series[str]\n",
        "        stop_words = set(stopwords.words('english'))  # Load stopwords\n",
        "        #if isinstance(text, str):\n",
        "        #    return [word for word in text.at[0, 'content'] if not word.lower() in stop_words]\n",
        "        #return text  # Return unchanged if not a string\n",
        "        return text[~text.isin(stop_words)]\n",
        "\n",
        "    ps = PorterStemmer()\n",
        "    def dataStemming_help(text):\n",
        "        #if isinstance(text, str):\n",
        "        #    return ps.stem(text)\n",
        "        #return text  # Return unchanged if not a string\n",
        "        if(isinstance(text, str)):\n",
        "            return pd.Series(ps.stem(text))\n",
        "        return text.apply(ps.stem)\n",
        "\n",
        "    def type_cleaner(text):\n",
        "        if isinstance(text, str):\n",
        "            return pd.Series(text).to_list()\n",
        "        return text.to_list()\n",
        "\n",
        "    data[column] = data[column].parallel_apply(apply_sequential_helper(\n",
        "        [clean_text_help, # str -> str\n",
        "        tokenize_text_help, # str -> list[str]\n",
        "        remove_stopwords_help, #series[str] -> series[str]\n",
        "        dataStemming_help, #series[str] -> series[str]\n",
        "        type_cleaner # series[str] -> series[str]\n",
        "    ]))\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "bf68c788a3d9437383fdf54f8f75f050",
            "13df6b96a88944e98d65e813eeb54bd4",
            "dc1dd2e88478465e8ebc8082b1aa84aa",
            "2814e4c7aa32487e93e3ccd90b6e5f0c",
            "3cc027a216d2481ab36faca98212f3c5",
            "4ae5f28587fe4d01ad03446e234a631d",
            "e28976f80c1548f2af7836d8e8491c87",
            "23088918d37a416a8506b3fd4c7231f9",
            "24f8c63d4dc3428fbcd67efc20c076f2",
            "6cfbf875045943ad8b40671a67e0e29e"
          ]
        },
        "id": "jy3LEMwF7dof",
        "outputId": "ca281a39-240a-4c1a-945c-8be4a96474db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['sometim', 'power', 'christma', 'make', 'wild', 'wonder', 'thing', 'need', 'believ', 'holi', 'triniti', 'believ', 'posit', 'power', 'good', 'other', 'simpl', 'act', 'give', 'without', 'receiv', 'lost', 'mani', 'us', 'day', 'worri', 'money', 'success', 'hold', 'us', 'back', 'give', 'other', 'need', 'one', 'congreg', 'ohio', 'move', 'action', 'power', 'sermon', 'given', 'church', 'christma', 'eve', 'pastor', 'grand', 'lake', 'unit', 'methodist', 'church', 'celina', 'ohio', 'gave', 'emot', 'sermon', 'import', 'understand', 'messag', 'jesu', 'mani', 'religi', 'peopl', 'messag', 'jesu', 'help', 'other', 'make', 'sure', 'peopl', 'suffer', 'get', 'help', 'need', 'enjoy', 'life', 'littl', 'bit', 'sermon', 'realli', 'generos', 'look', 'like', 'live', 'jesu', 'live', 'long', 'time', 'ago', 'act', 'gener', 'fashion', 'time', 'would', 'gener', 'act', 'look', 'like', 'time', 'focu', 'sermon', 'potenc', 'sermon', 'lost', 'congreg', 'move', 'take', 'action', 'sermon', 'end', 'congreg', 'decid', 'take', 'offer', 'bowl', 'pass', 'around', 'room', 'everyon', 'pitch', 'could', 'christma', 'eve', 'word', 'sermon', 'still', 'ring', 'ear', 'offer', 'member', 'congreg', 'drove', 'local', 'waffl', 'hous', 'visit', 'ladi', 'work', 'night', 'shift', 'great', 'choic', 'holi', 'day', 'everyon', 'famili', 'ladi', 'work', 'waffl', 'hous', 'clearli', 'famili', 'choic', 'work', 'holi', 'day', 'paid', 'bill', 'congreg', 'understood', 'sacrific', 'made', 'ladi', 'want', 'help', 'donat', 'entir', 'offer', 'split', 'amongst', 'ladi', 'waffl', 'hous', 'total', 'amount', '$', 'num', 'split', 'amongst', 'staff', 'beauti', 'moment', 'perfect', 'exampl', 'preacher', 'talk', 'sermon', 'good', 'deed', 'like', 'christma', 'realli', 'help', 'eas', 'burden', 'felt', 'ladi', 'work', 'waffl', 'hous', 'sure', 'could', 'see', 'famili', 'least', 'got', 'littl', 'gift', 'good', 'peopl', 'commun', 'perhap', 'best', 'part', 'whole', 'event', 'congreg', 'ask', 'anyth', 'return', 'simpl', 'act', 'generos', 'peopl', 'understood', 'pain', 'felt', 'anoth', 'group', 'sought', 'allevi', 'pain', 'speak', 'volum', 'merit', 'church', 'daili', 'live', 'simpl', 'act', 'brought', 'entir', 'commun', 'togeth', 'show', 'empathi', 'compass', 'special', 'day', 'year']\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "nsdf_cleaned = cleanText(nsdf, 'content')\n",
        "nsdf_tokenized = tokenizeText(nsdf_cleaned, 'content')                  #tokenizing\n",
        "nsdf_cleaned_tokenized_nostopwords = remove_stopwords(nsdf_tokenized, 'content')           #removing stopwords\n",
        "nsdf_preprocessed = dataStemming(nsdf_cleaned_tokenized_nostopwords, 'content')\n",
        "\"\"\"\n",
        "nsdf_processed = processData(nsdf, 'content')\n",
        "nsdf_processed.dropna(subset=['content'], inplace=True)  # Drop rows with no content\n",
        "nsdf_processed.reset_index(drop=True, inplace=True)  # Reset index\n",
        "print(nsdf_processed.at[0, 'content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4U9DbWw7dof",
        "outputId": "fe4d12d1-ee80-4786-a87c-c8619f573c2a"
      },
      "outputs": [],
      "source": [
        "#word frequency pre preprocessing\n",
        "# print(\"word frequency pre preprocessing\")\n",
        "# word_frequency_pre = getFreq(nsdf_raw, 'content').sum()\n",
        "# print(word_frequency_pre)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoqJWce17dog",
        "outputId": "73b36b5b-a812-46a6-823b-f7fc8159703f"
      },
      "outputs": [],
      "source": [
        "#word frequency post preprocessing\n",
        "# print(\"word frequency post preprocessing\")\n",
        "# getfreq assumes different type signature, so we have to do it this way\n",
        "# word_frequency_post = nsdf_processed[\"content\"].explode().value_counts().sum()\n",
        "# print(word_frequency_post)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5uKU6le7dog"
      },
      "outputs": [],
      "source": [
        "#word frequency post stemming\n",
        "#print(\"word frequency post stemming\")\n",
        "#word_frequency_postStem = getFrequency(nsdf_.processed, 'content', 1)\n",
        "#print(sum(word_frequency_postStem.values()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1OIqoGT7dog"
      },
      "outputs": [],
      "source": [
        "#pre = word_frequency_pre\n",
        "#post = word_frequency_post\n",
        "#print(\"Reduction rate of the vocabulary size after removing stopwords:\")\n",
        "#print(abs(pre -post))\n",
        "#print(\"Further  reduction rate of the vocabulary size after stemming\")\n",
        "#postStem = sum(word_frequency_post.values())\n",
        "#print(abs(post -postStem))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1czxzP-7dog"
      },
      "source": [
        "### Task 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#load data\n",
        "fakeNewsCorpus = pd.read_csv(dataPath + \"995,000_rows.csv\")\n",
        "#Hva saten er den der unnamed???\n",
        "print(fakeNewsCorpus.head())\n",
        "#fakeNewsCorpus['content'].duplicated()\n",
        "news_noDup = fakeNewsCorpus.drop_duplicates(subset=['content']).dropna(subset=['content']).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9fd9741bc796445dacef77f07614167e",
            "1fdcaf3f9a5245eea4a903b928eee5b8",
            "e347b4d8b9fb4c72929f6a646167747c",
            "b58df1204e824dbd9f72cdb606b2605f",
            "b1c6087d3824401aa91057fe441302fe",
            "830cda2af3cb49ecbfe45ca7f2e04a24",
            "6e08915023cb458abda3abdce421467e",
            "7258d6e2da3a41ad9b4830b0192cb037",
            "d9249597abf942b5af7a16e2ebefa329",
            "a24a494c9ffb43e1bfe493d1162c4e3d"
          ]
        },
        "id": "u-6EvFOO7dog",
        "outputId": "dcfc62f5-5370-44df-c102-5d896416d2e3"
      },
      "outputs": [],
      "source": [
        "#Cleaning\n",
        "news_processed = processData(news_noDup, 'content')\n",
        "news_processed.to_json(dataPath + \"news_processed.json\", orient='records', lines=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCHjcIp97dog"
      },
      "source": [
        "### Task 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsAEJIoO7dog",
        "outputId": "f7b8a93f-b748-4f47-c421-13a928bff4dc"
      },
      "outputs": [],
      "source": [
        "if news_processed is None:\n",
        "    json_reader = pd.read_json(dataPath + \"news_processed.json\", orient='records', lines=True, chunksize=1500)\n",
        "    news_processed = pd.concat(json_reader, ignore_index=True)\n",
        "# timed: 12 min på M1 macbook chunk=1000\n",
        "print(news_processed.info())   # Check column types and missing values\n",
        "#fndf = fakeNewsCorpus.reset_index(drop=True)  # Reset index\n",
        "fndf = news_processed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRVr6AxZ7dog",
        "outputId": "9013e78e-a2bf-4f2d-d54d-499d3dde514b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqKMjLFX7doh"
      },
      "source": [
        "### Cleaning and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOn3autl7doh",
        "outputId": "b049f942-d2ac-4c2c-d85b-0d7c8392e567"
      },
      "outputs": [],
      "source": [
        "print(\"Pandas DataFrame:\")\n",
        "display(fndf.iloc[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Observations about dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unique_values = fndf['type'].unique()\n",
        "print(unique_values)\n",
        "#hard to know how to classify nan and unknown, so removed for now\n",
        "# we also remove the a weird type \n",
        "fndf = fndf.dropna(subset=['type'])\n",
        "fndf = fndf.loc[(fndf['type']!='unknown') & (fndf['type']!='unreliable') & (fndf['type'] != '2018-02-10 13:43:39.521661')]\n",
        "# Need to reset index\n",
        "\n",
        "newunique_values = fndf['type'].unique()\n",
        "print(newunique_values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# groups (reliable) as truenews 1 and (all others) in fakenews 0\n",
        "    #note this is naive and should be reconsidered later\n",
        "\n",
        "# reliable, clickbait and political are all, by their contents, factually correct (albeit possibly politcally motivated)\n",
        "# we deem those to be \"real\" news\n",
        "print(\"adding binary labels\")\n",
        "fndf['type'] = fndf['type'].replace(r'^(reliable|clickbait|political)$', '1', regex=True) \n",
        "fndf['type'] = fndf['type'].replace(r'^(?!1$).+', '0', regex=True)   # Replace everything except '1' with '0'\n",
        "#fndf['type'] = fndf['type'].fillna('0')\n",
        "\n",
        "newunique_values = fndf['type'].unique()\n",
        "print(newunique_values)\n",
        "fndf.shape[0]\n",
        "fndf['type'] = fndf['type'].astype(int)  # Convert to integer\n",
        "\n",
        "print((\"real vs fake:\"))\n",
        "print(fndf['type'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "relib_news = fndf.loc[fndf['type'] == 1]\n",
        "fake_news = fndf.loc[fndf['type'] == 0]\n",
        "\n",
        "print(relib_news.shape[0], \" : \", fake_news.shape[0])\n",
        "\n",
        "print(\"distribution for real news\")\n",
        "#print(relib_news[\"content\"].explode().value_counts())\n",
        "real_dist = FreqDist(relib_news[\"content\"].explode())\n",
        "print(real_dist.most_common(200))\n",
        "\n",
        "print(\"distribution for fake news\")\n",
        "#print(fake_news[\"content\"].explode().value_counts())\n",
        "fake_dist = FreqDist(fake_news[\"content\"].explode())\n",
        "print(fake_dist.most_common(200))\n",
        "\n",
        "print(\"distribution for all news\")\n",
        "#print(fndf[\"content\"].explode().value_counts())\n",
        "all_dist = FreqDist(fndf[\"content\"].explode())\n",
        "print(all_dist.most_common(200))\n",
        "joblib.dump((real_dist, fake_dist, all_dist), dataPath + \"dists.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "(real_dist, fake_dist, all_dist) = joblib.load(dataPath + \"dists.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "fake_total = int(pd.Series([b for (a, b) in fake_dist.most_common(1000)]).sum())\n",
        "real_total = int(pd.Series([b for (a, b) in real_dist.most_common(1000)]).sum())\n",
        "\n",
        "print(f\"real total: {real_total}, fake total: {fake_total}\")\n",
        "print(real_total-fake_total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "real_pd = pd.Series(dict(real_dist.most_common(1000))).apply(lambda x: math.log2(x/real_total))\n",
        "fake_pd = pd.Series(dict(fake_dist.most_common(1000))).apply(lambda x: math.log2(x/fake_total))\n",
        "real_set = set(real_pd.index)\n",
        "fake_set = set(fake_pd.index)\n",
        "unique_words = real_set.union(fake_set) - real_set.intersection(fake_set)\n",
        "print(f\"Amount of words only present in one set than another: {len(unique_words)}\")\n",
        "\n",
        "large_diff = [(word, float(real_pd.get(word, 0) - fake_pd.get(word, 0))) for word in fake_pd.index if abs(real_pd.get(word, 0) - fake_pd.get(word, 0)) > 5]\n",
        "large_diff.sort(key=lambda x: x[1], reverse=True)\n",
        "print(large_diff)\n",
        "\n",
        "print(\"Unique tokens:\")\n",
        "for token in [\"url\", \"email\", \"num\", \"date\"]:\n",
        "    print(f\"TOKEN: {token} - Real: {real_pd[token]}, Fake: {fake_pd[token]}\")\n",
        "\n",
        "plt.bar(real_pd.keys(), real_pd.values)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEK4r7cy7doh"
      },
      "source": [
        "## Task 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxlKCVeK7doh",
        "outputId": "689ae9da-ca85-41ac-877b-dd07c1b5d147"
      },
      "outputs": [],
      "source": [
        "# Splitting into test, train and validation\n",
        "X_train_full, X_valtest_full, y_train, y_valtest = train_test_split(fndf, fndf['type'], test_size=0.2, random_state=42) # Replace with fakenews dataset for real linreg.\n",
        "X_test_full, X_val_full, y_test, y_val = train_test_split(X_valtest_full, y_valtest, test_size=0.5, random_state=42)\n",
        "# x_train = testing_ x, y_train = training_y\n",
        "# (x_test\n",
        "X_train = X_train_full['content']\n",
        "X_test = X_test_full['content']\n",
        "print(\"train size:\", y_train.shape)\n",
        "print(\"val size:\", y_val.shape)\n",
        "print(\"test size:\", y_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1MTfwNy7doh"
      },
      "source": [
        "# Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 0, splitting labels into reliable and unreliable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "unique_values = fndf['type'].unique()\n",
        "print(unique_values)\n",
        "#hard to know how to classify nan and unknown, so removed for now\n",
        "# we also remove the a weird type \n",
        "fndf = fndf.dropna(subset=['type'])\n",
        "fndf = fndf.loc[fndf['type']!='unknown']\n",
        "# Need to reset index\n",
        "\n",
        "newunique_values = fndf['type'].unique()\n",
        "print(newunique_values)\n",
        "\n",
        "\n",
        "# groups (reliable) as truenews 1 and (all others) in fakenews 0\n",
        "    #note this is naive and should be reconsidered later\n",
        "\n",
        "# reliable, clickbait and political are all, by their contents, factually correct (albeit possibly politcally motivated)\n",
        "# we deem those to be \"real\" news\n",
        "fndf['type'] = fndf['type'].replace(r'^(reliable|clickbait|political)$', '1', regex=True) \n",
        "fndf['type'] = fndf['type'].replace(r'^(?!1$).+', '0', regex=True)   # Replace everything except '1' with '0'\n",
        "#fndf['type'] = fndf['type'].fillna('0')\n",
        "\n",
        "newunique_values = fndf['type'].unique()\n",
        "print(newunique_values)\n",
        "fndf.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1 - Simple linear regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get the top 10000 words, and how often they occur in each article"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Standarize fndf?\n",
        "\n",
        "# get top 10000 words for vocab in training data to avoid leaking data from test set\n",
        "print(\"Finding vocabulary:\")\n",
        "vocab = X_train.explode().value_counts()[:10000].keys()\n",
        "#training_split_freq = FreqDist(X_train.explode())\n",
        "#vocab = [x for (x, y) in training_split_freq.most_common(10000)]\n",
        "#print(pd.Series(nsdf[\"content\"][0]).value_counts())\n",
        "#print(pd.Series(fndf[\"content\"][0]).value_counts()[top.keys()[0]])\n",
        "\n",
        "def CountFreq(words, vocab):\n",
        "    # words: Series[str]\n",
        "    # vocab: list[str] of words to count\n",
        "    words = pd.Series(words)\n",
        "    new_row = pd.Series(np.zeros(len(vocab)))\n",
        "    n = len(vocab)\n",
        "    words_in_article = pd.Series(words).value_counts()\n",
        "    return vocab.apply(lambda x: words_in_article.get(x, 0))\n",
        "\n",
        "#rowsFreq = X_train.parallel_apply(lambda row: CountFreq(row, pd.Series(top.keys())))\n",
        "# her bruger vi forskellige vocabularies - vi skal finde vocab for træning og apply den til test_rowsFreq\n",
        "vectorizer = CountVectorizer(analyzer=lambda x: x, vocabulary=vocab)\n",
        "print(\"vectorizing X_train\")\n",
        "rowsFreq = vectorizer.fit_transform(X_train)\n",
        "print(\"vectorizing X_test\")\n",
        "test_rowsFreq = vectorizer.fit_transform(X_test)\n",
        "print(rowsFreq)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#store data\n",
        "\n",
        "joblib.dump((rowsFreq, test_rowsFreq), dataPath + \"rowsFreq.pkl\")\n",
        "joblib.dump((y_train, y_test, y_val), dataPath + \"y.pkl\")\n",
        "#joblib.dump((x_undersampled, y_undersampled), dataPath + \"undersampled.pkl\")\n",
        "joblib.dump((X_train_full, X_test_full, X_val_full), dataPath + \"X_full.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating the linear regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#scaler = StandardScaler(with_mean=False)\n",
        "scaler = Normalizer()\n",
        "undersampler = RandomUnderSampler(random_state=42)\n",
        "\n",
        "X_train_scale = scaler.fit_transform(rowsFreq)\n",
        "X_test_scale = scaler.transform(test_rowsFreq)\n",
        "x_undersampled, y_undersampled = undersampler.fit_resample(X_train_scale, y_train)\n",
        "print(\"Starting model training:\")\n",
        "linReg = LogisticRegression(max_iter=1000, penalty=\"l1\", solver='liblinear', random_state=42)#, class_weight=\"balanced\")\n",
        "linReg_weighed = LogisticRegression(max_iter=1000, penalty=\"l1\", solver='liblinear', random_state=42, class_weight=\"balanced\")\n",
        "linReg_undersampled = LogisticRegression(max_iter=1000, penalty=\"l1\", solver='liblinear', random_state=42)\n",
        "print(\"fitting standard model\")\n",
        "linReg.fit(X_train_scale, y_train)\n",
        "print(\"fitting weighted model\")\n",
        "linReg_weighed.fit(X_train_scale, y_train)\n",
        "print(\"fitting undersampled model\")\n",
        "linReg_undersampled.fit(x_undersampled, y_undersampled)\n",
        "\n",
        "y_pred = linReg.predict(X_test_scale)\n",
        "y_pred_weighted = linReg_weighed.predict(X_test_scale)\n",
        "y_pred_undersampled = linReg_undersampled.predict(X_test_scale)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "f1_weighted = f1_score(y_test, y_pred_weighted)\n",
        "f1_undersampled = f1_score(y_test, y_pred_undersampled)\n",
        "# Print results\n",
        "print(\"Standard : Weighted : Undersampled\")\n",
        "print(f\"F1 Score: {f1:.4f} : {f1_weighted:.4f} : {f1_undersampled:.4f}\")\n",
        "print(f\"Hyperparameters: max_iter=1000, solver='liblinear', binary bag-of-words\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model\n",
        "joblib.dump((linReg, linReg_weighed, linReg_undersampled), dataPath + \"linReg.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reload data \n",
        "(linReg, linReg_weighed, linReg_undersampled) = joblib.load(dataPath + \"linReg.pkl\")\n",
        "(rowsFreq, test_rowsFreq) = joblib.load(dataPath + \"rowsFreq.pkl\")\n",
        "(y_train, y_test, y_val) = joblib.load(dataPath + \"y.pkl\")\n",
        "#(x_undersampled, y_undersampled) = joblib.load(dataPath + \"undersampled.pkl\")\n",
        "\n",
        "scaler = Normalizer()\n",
        "X_train_scale = scaler.fit_transform(rowsFreq)\n",
        "X_test_scale = scaler.transform(test_rowsFreq)\n",
        "\n",
        "y_pred = linReg.predict(X_test_scale)\n",
        "y_train_pred = linReg.predict(X_train_scale)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Visualize\n",
        "print(f\"F1 Score: Train: {f1_score(y_train, y_train_pred):.4f}, Test: {f1_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Accuracy (test): {accuracy_score(y_test, y_pred):.4f}\")\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=linReg.classes_)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bbc_raw = pd.from_csv(dataPath + \"bbc_articles.csv\")\n",
        "\n",
        "bbc_processed = processData(bbc_raw, 'text')\n",
        "bbc_processed['type'] = 1\n",
        "\n",
        "bbc_processed.to_json(dataPath + \"bbc_processed.json\", orient='records', lines=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if bbc_proccessed is None:\n",
        "    json_reader = pd.read_json(dataPath + \"bbc_processed.json\", orient='records', lines=True, chunksize=1500)\n",
        "    bbc_processed = pd.concat(json_reader, ignore_index=True)\n",
        "\n",
        "bbcdf = bbc_processed\n",
        "\n",
        "BBC_train = pd.concat([X_train, bbcdf['text']]).reset_index(drop=True)\n",
        "BBC_train = scaler.fit_transform(BBC_train)\n",
        "BBC_y = pd.concat([y_train, bbcdf[\"type\"]])\n",
        "bbcReg = LogisticRegression(max_iter=1000, penalty=\"l1\", solver='liblinear', random_state=42)\n",
        "bbcReg.fit(BBC_train, BBC_y)\n",
        "BBC_pred = bbcReg.predict(X_test)\n",
        "print(f\"F1: {f1_score(y_test, BBC_pred)}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "gds-fnproj",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "13df6b96a88944e98d65e813eeb54bd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2814e4c7aa32487e93e3ccd90b6e5f0c",
              "IPY_MODEL_3cc027a216d2481ab36faca98212f3c5"
            ],
            "layout": "IPY_MODEL_4ae5f28587fe4d01ad03446e234a631d"
          }
        },
        "1fdcaf3f9a5245eea4a903b928eee5b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b58df1204e824dbd9f72cdb606b2605f",
              "IPY_MODEL_b1c6087d3824401aa91057fe441302fe"
            ],
            "layout": "IPY_MODEL_830cda2af3cb49ecbfe45ca7f2e04a24"
          }
        },
        "23088918d37a416a8506b3fd4c7231f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "24f8c63d4dc3428fbcd67efc20c076f2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2814e4c7aa32487e93e3ccd90b6e5f0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "IntProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100.00%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e28976f80c1548f2af7836d8e8491c87",
            "max": 232,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_23088918d37a416a8506b3fd4c7231f9",
            "value": 232
          }
        },
        "3cc027a216d2481ab36faca98212f3c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24f8c63d4dc3428fbcd67efc20c076f2",
            "placeholder": "​",
            "style": "IPY_MODEL_6cfbf875045943ad8b40671a67e0e29e",
            "value": "232 / 232"
          }
        },
        "4ae5f28587fe4d01ad03446e234a631d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cfbf875045943ad8b40671a67e0e29e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e08915023cb458abda3abdce421467e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7258d6e2da3a41ad9b4830b0192cb037": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "830cda2af3cb49ecbfe45ca7f2e04a24": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fd9741bc796445dacef77f07614167e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1fdcaf3f9a5245eea4a903b928eee5b8"
            ],
            "layout": "IPY_MODEL_e347b4d8b9fb4c72929f6a646167747c"
          }
        },
        "a24a494c9ffb43e1bfe493d1162c4e3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1c6087d3824401aa91057fe441302fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9249597abf942b5af7a16e2ebefa329",
            "placeholder": "​",
            "style": "IPY_MODEL_a24a494c9ffb43e1bfe493d1162c4e3d",
            "value": "0 / 812913"
          }
        },
        "b58df1204e824dbd9f72cdb606b2605f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "IntProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "0.00%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e08915023cb458abda3abdce421467e",
            "max": 812913,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7258d6e2da3a41ad9b4830b0192cb037",
            "value": 0
          }
        },
        "bf68c788a3d9437383fdf54f8f75f050": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_13df6b96a88944e98d65e813eeb54bd4"
            ],
            "layout": "IPY_MODEL_dc1dd2e88478465e8ebc8082b1aa84aa"
          }
        },
        "d9249597abf942b5af7a16e2ebefa329": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc1dd2e88478465e8ebc8082b1aa84aa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e28976f80c1548f2af7836d8e8491c87": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e347b4d8b9fb4c72929f6a646167747c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
