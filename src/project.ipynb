{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afP0fI0WAjLQ",
        "outputId": "63969ef1-021e-4875-bd33-e77080113061"
      },
      "outputs": [],
      "source": [
        "# prompt: please install all the below using pip\n",
        "\n",
        "!pip install numpy pandas matplotlib seaborn scikit-learn tensorflow keras clean_text pandarallel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7I5ZeAs7doa",
        "outputId": "f04e908c-53cf-4e5e-b1ac-6c4eeba64133"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gensim\n",
        "import logging\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "\n",
        "import seaborn as sns\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.lm import Vocabulary\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.preprocessing import StandardScaler, Normalizer\n",
        "from sklearn.decomposition import PCA\n",
        "import joblib\n",
        "from cleantext import clean\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.stem import PorterStemmer\n",
        "from pandarallel import pandarallel\n",
        "import ast\n",
        "import math\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "pandarallel.initialize(progress_bar=True)\n",
        "import time\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "import os\n",
        "IN_COLAB = False\n",
        "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
        "   IN_COLAB = True\n",
        "\n",
        "news_processed = None\n",
        "bbc_proccessed = None\n",
        "dataPath = \"../data/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DWSzbnj75b9",
        "outputId": "79c1884d-37d0-4041-ee74-66a39de326ab"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWp-AaTm7doc"
      },
      "source": [
        "# Part 1 (Data Processing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAkg7frj7dod"
      },
      "source": [
        "### Task 1\n",
        "Retrive data sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0lGjcG17dod",
        "outputId": "9d148c02-9df3-4945-ad46-60fa24a7a724"
      },
      "outputs": [],
      "source": [
        "# Data path\n",
        "dataPath = \"../data/\"\n",
        "if IN_COLAB:\n",
        "  dataPath = \"/content/drive/MyDrive/\"\n",
        "\n",
        "# Read data from csv using pandas\n",
        "nsdf = pd.read_csv(dataPath + \"news_sample.csv\")\n",
        "nsdf = nsdf.reset_index(drop=True)  # Reset index??\n",
        "\n",
        "# Safe raw data\n",
        "nsdf_raw = nsdf.copy(deep=True)\n",
        "\n",
        "nsdf.info()   # Check column types and missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw3NaRAQ7doe",
        "outputId": "40b81a48-d87e-44ce-cc4d-775c58a7663b"
      },
      "outputs": [],
      "source": [
        "# unique lable values\n",
        "unique_values = nsdf['type'].unique()\n",
        "print(unique_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Sr01Nsx7doe",
        "outputId": "f841641a-6f2a-4902-e47e-ddd941ae53dc"
      },
      "outputs": [],
      "source": [
        "#nan and unknown removed as they seem useless when training a classifier\n",
        "nsdf = nsdf.dropna(subset=['type'])\n",
        "nsdf = nsdf.loc[nsdf['type']!='unknown']\n",
        "newunique_values = nsdf['type'].unique()\n",
        "print(newunique_values)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bdqH5Za7doe"
      },
      "source": [
        "#### Cleaning and Preprocessing functions\n",
        "\n",
        "To test the different functions and see the reduction in vocabularity size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aqMgikT7doe"
      },
      "outputs": [],
      "source": [
        "def cleanText(data, column):\n",
        "    data[column] = data[column].parallel_apply(clean_text_help)\n",
        "    return data\n",
        "\n",
        "def clean_text_help(text):\n",
        "    if isinstance(text, str):\n",
        "        # Remove excess whitespace\n",
        "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "        #replace dates\n",
        "        text = re.sub(r\"(0[1-9]|[1-2][0-9]|3[0-1])[-/.]?(0[1-9]|1[0-2])[-/.]?([0-9]{2}|[0-9]{4})\", \"<DATE>\", text)  # Replace date type 1\n",
        "        text = re.sub(r\"(0[1-9]|[1-2][0-9]|3[0-1])\\s([A-Za-z]{3})\\s([0-9]{2}|[0-9]{4})\", \"<DATE>\", text)  # Replace date type 2\n",
        "        return clean(text, lower=True, no_line_breaks=True, no_numbers=True, no_emails=True, no_urls=True, no_punct=True, replace_with_url=r\"__URL__\", replace_with_email=r\"__EMAIL__\", replace_with_number=r\"__NUM__\", replace_with_digit=r\"__NUM__\")\n",
        "    raise TypeError(\"Clean_text passed non-string\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCu5do7_7doe"
      },
      "outputs": [],
      "source": [
        "#Tokenize the text function\n",
        "def tokenizeText(data, column):\n",
        "    def tokenize_text_help(text):\n",
        "        if isinstance(text, str):\n",
        "            return (word_tokenize(text))\n",
        "        return text  # Return unchanged if not a string\n",
        "    data[column] = data[column].parallel_apply(tokenize_text_help)  # Apply function\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NieKcD1f7dof"
      },
      "outputs": [],
      "source": [
        "#function for removeing stopwords\n",
        "def remove_stopwords_help(text):\n",
        "    text = pd.Series(text)\n",
        "    stop_words = set(stopwords.words('english'))  # Load stopwords\n",
        "    return text[~text.isin(stop_words)].to_list()  # Remove stopwords\n",
        "\n",
        "def remove_stopwords(data, column):\n",
        "    data['content'] = data['content'].parallel_apply(remove_stopwords_help)  # Apply function\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HPQfCwE7dof"
      },
      "outputs": [],
      "source": [
        "#Returns a pandas series, with word and frequency, very fast.\n",
        "def getFreq(data, column):\n",
        "    return len(data[column].str.split().explode().value_counts())\n",
        "\n",
        "def getFreq_tokinized(data, column):\n",
        "    return len(data[column].explode().value_counts())\n",
        "\n",
        "def Vocab_size_tokinized(data, column):\n",
        "    return len(data[column].explode())\n",
        "\n",
        "def Vocab_size(data, column):\n",
        "    return len(data[column].str.split().explode())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjoX2dVY7dof"
      },
      "outputs": [],
      "source": [
        "\n",
        "#function for removeing stopwords\n",
        "def dataStemming(data, column):\n",
        "    ps = PorterStemmer()\n",
        "    def dataStemming_help(text):\n",
        "        text = pd.Series(text)\n",
        "        if(isinstance(text, str)):\n",
        "            return pd.Series(ps.stem(text))\n",
        "        return text.apply(ps.stem).to_list()\n",
        "    data[column] = data[column].parallel_apply(dataStemming_help)  # Apply function\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_reduction(start_value, end_value):\n",
        "    return round(((start_value - end_value)/start_value)*100, 2)\n",
        "\n",
        "\n",
        "printing_copy = nsdf_raw.copy(deep=True)\n",
        "freq_raw = getFreq(printing_copy, 'content')\n",
        "data_size_raw = Vocab_size(printing_copy, 'content')\n",
        "\n",
        "printing_copy = printing_copy.dropna(subset=['type'])\n",
        "printing_copy = printing_copy.loc[printing_copy['type']!='unknown']\n",
        "\n",
        "freq_pre = getFreq(printing_copy, 'content')\n",
        "data_size_pre = Vocab_size(printing_copy, 'content')\n",
        "freq_clean = getFreq(cleanText(printing_copy, 'content'), 'content')\n",
        "data_size_clean = Vocab_size(printing_copy, 'content')\n",
        "freq_token = getFreq_tokinized(tokenizeText(printing_copy, 'content'), 'content')\n",
        "data_size_token = Vocab_size_tokinized(printing_copy, 'content')\n",
        "freq_stopwords = getFreq_tokinized(remove_stopwords(printing_copy, 'content'), 'content')\n",
        "data_size_stopwords = Vocab_size_tokinized(printing_copy, 'content')\n",
        "freq_stemmed = getFreq_tokinized(dataStemming(printing_copy, 'content'), 'content')\n",
        "data_size_stemmed = Vocab_size_tokinized(printing_copy, 'content')\n",
        "\n",
        "\n",
        "print(\"--- Unique words ---\")\n",
        "print(\"Raw: \", freq_raw)\n",
        "print(\"Preprocessing: \", freq_pre, \"reduction: \", calc_reduction(freq_raw, freq_pre))\n",
        "print(\"After cleaning: \", freq_clean, \"reduction: \", calc_reduction(freq_raw, freq_clean))\n",
        "print(\"After tokenizing: \", freq_token, \"reduction: \", calc_reduction(freq_raw, freq_token))\n",
        "print(\"After removing stopwords: \", freq_stopwords, \"reduction: \", calc_reduction(freq_raw, freq_stopwords))\n",
        "print(\"After stemming: \", freq_stemmed, \"reduction: \", calc_reduction(freq_raw, freq_stemmed))\n",
        "\n",
        "print(\" \")\n",
        "\n",
        "print(\"--- Word counts ---\")\n",
        "print(\"Raw: \", data_size_raw)\n",
        "print(\"Preprosseing: \", data_size_pre, \"reduction: \", calc_reduction(data_size_raw, data_size_pre))\n",
        "print(\"After cleaning: \", data_size_clean, \"reduction: \", calc_reduction(data_size_raw, data_size_clean))\n",
        "print(\"After tokenizing: \", data_size_token, \"reduction: \", calc_reduction(data_size_raw, data_size_token))\n",
        "print(\"After removing stopwords: \", data_size_stopwords, \"reduction: \", calc_reduction(data_size_raw, data_size_stopwords))\n",
        "print(\"After stemming: \", data_size_stemmed, \"reduction: \", calc_reduction(data_size_raw, data_size_stemmed))\n",
        "\n",
        "\n",
        "#Delete copy:\n",
        "del printing_copy\n",
        "del freq_pre, data_size_pre, freq_clean, data_size_clean, freq_token, data_size_token, freq_stopwords, data_size_stopwords, freq_stemmed, data_size_stemmed\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Processing and cleaning function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGqT6iu77dof"
      },
      "outputs": [],
      "source": [
        "# One big function to process data: uses functional programming approach to simplify changes\n",
        "def processData(data, column, stemming=True):\n",
        "    def apply_sequential_helper(functions):\n",
        "        # assume type siganture of functions to be List[f : String -> string ]\n",
        "        def inner(text):\n",
        "            for f in functions:\n",
        "                text = f(text)\n",
        "            return text\n",
        "        return inner\n",
        "\n",
        "    def clean_text_help(text):\n",
        "        if isinstance(text, str):\n",
        "            # Remove excess whitespace\n",
        "            text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "            # remove non-ascii\n",
        "            text = re.sub(r'[^\\w _]+', '', text)\n",
        "            \n",
        "            #replace dates\n",
        "            text = re.sub(r\"(0[1-9]|[1-2][0-9]|3[0-1])[-/.]?(0[1-9]|1[0-2])[-/.]?([0-9]{2}|[0-9]{4})\", \" __DATE__ \", text)  # Replace date type 1\n",
        "            text = re.sub(r\"(0[1-9]|[1-2][0-9]|3[0-1])\\s([A-Za-z]{3})\\s([0-9]{2}|[0-9]{4})\", \" __DATE__ \", text)  # Replace date type 2\n",
        "\n",
        "            return clean(text, lower=True, no_line_breaks=True, no_currency_symbols=True, no_numbers=True, \n",
        "                         no_emails=True, no_urls=True, no_punct=True, replace_with_url=r\" __URL__ \", replace_with_email=r\" __EMAIL__ \", \n",
        "                         replace_with_number=r\" __NUM__ \", replace_with_digit=r\" __NUM__ \", replace_with_currency_symbol=r\" __CUR__ \")\n",
        "        raise TypeError(\"Clean_text passed non-string\")\n",
        "\n",
        "    def tokenize_text_help(text):\n",
        "        if isinstance(text, str):\n",
        "            return pd.Series(word_tokenize(text))\n",
        "        return text  # Return unchanged if not a string\n",
        "\n",
        "    def remove_stopwords_help(text):\n",
        "      # text is a Series[str]\n",
        "        stop_words = set(stopwords.words('english'))  # Load stopwords\n",
        "        #if isinstance(text, str):\n",
        "        #    return [word for word in text.at[0, 'content'] if not word.lower() in stop_words]\n",
        "        #return text  # Return unchanged if not a string\n",
        "        return text[~text.isin(stop_words)]\n",
        "\n",
        "    ps = PorterStemmer()\n",
        "    def dataStemming_help(text):\n",
        "        #if isinstance(text, str):\n",
        "        #    return ps.stem(text)\n",
        "        #return text  # Return unchanged if not a string\n",
        "        if(isinstance(text, str)):\n",
        "            return pd.Series(ps.stem(text))\n",
        "        return text.apply(ps.stem)\n",
        "\n",
        "    def type_cleaner(text):\n",
        "        if isinstance(text, str):\n",
        "            return pd.Series(text).to_list()\n",
        "        return text.to_list()\n",
        "\n",
        "    data[column] = data[column].parallel_apply(apply_sequential_helper(\n",
        "        [clean_text_help, # str -> str\n",
        "        tokenize_text_help, # str -> list[str]\n",
        "        remove_stopwords_help, #series[str] -> series[str]\n",
        "        dataStemming_help if stemming else lambda x:x, #series[str] -> series[str]\n",
        "        type_cleaner # series[str] -> series[str]\n",
        "    ]))\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "bf68c788a3d9437383fdf54f8f75f050",
            "13df6b96a88944e98d65e813eeb54bd4",
            "dc1dd2e88478465e8ebc8082b1aa84aa",
            "2814e4c7aa32487e93e3ccd90b6e5f0c",
            "3cc027a216d2481ab36faca98212f3c5",
            "4ae5f28587fe4d01ad03446e234a631d",
            "e28976f80c1548f2af7836d8e8491c87",
            "23088918d37a416a8506b3fd4c7231f9",
            "24f8c63d4dc3428fbcd67efc20c076f2",
            "6cfbf875045943ad8b40671a67e0e29e"
          ]
        },
        "id": "jy3LEMwF7dof",
        "outputId": "ca281a39-240a-4c1a-945c-8be4a96474db"
      },
      "outputs": [],
      "source": [
        "\n",
        "nsdf_processed = processData(nsdf, 'content')\n",
        "nsdf_processed.dropna(subset=['content'], inplace=True)  # Drop rows with no content\n",
        "nsdf_processed.reset_index(drop=True, inplace=True)  # Reset index\n",
        "print(nsdf_processed.at[0, 'content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1czxzP-7dog"
      },
      "source": [
        "### Task 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#load data\n",
        "fakeNewsCorpus = pd.read_csv(dataPath + \"995,000_rows.csv\")\n",
        "#Hva saten er den der unnamed???\n",
        "print(fakeNewsCorpus.head())\n",
        "#fakeNewsCorpus['content'].duplicated()\n",
        "news_noDup = fakeNewsCorpus.drop_duplicates(subset=['content']).dropna(subset=['content']).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9fd9741bc796445dacef77f07614167e",
            "1fdcaf3f9a5245eea4a903b928eee5b8",
            "e347b4d8b9fb4c72929f6a646167747c",
            "b58df1204e824dbd9f72cdb606b2605f",
            "b1c6087d3824401aa91057fe441302fe",
            "830cda2af3cb49ecbfe45ca7f2e04a24",
            "6e08915023cb458abda3abdce421467e",
            "7258d6e2da3a41ad9b4830b0192cb037",
            "d9249597abf942b5af7a16e2ebefa329",
            "a24a494c9ffb43e1bfe493d1162c4e3d"
          ]
        },
        "id": "u-6EvFOO7dog",
        "outputId": "dcfc62f5-5370-44df-c102-5d896416d2e3"
      },
      "outputs": [],
      "source": [
        "#Cleaning\n",
        "news_processed = processData(news_noDup, 'content')\n",
        "news_processed.to_json(dataPath + \"news_processed.json\", orient='records', lines=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCHjcIp97dog"
      },
      "source": [
        "### Task 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsAEJIoO7dog",
        "outputId": "f7b8a93f-b748-4f47-c421-13a928bff4dc"
      },
      "outputs": [],
      "source": [
        "# avoid reprocessing\n",
        "if news_processed is None:\n",
        "    json_reader = pd.read_json(dataPath + \"news_processed.json\", orient='records', lines=True, chunksize=1500)\n",
        "    news_processed = pd.concat(json_reader, ignore_index=True)\n",
        "# timed: 12 min på M1 macbook chunk=1000\n",
        "print(news_processed.info())   # Check column types and missing values\n",
        "#fndf = fakeNewsCorpus.reset_index(drop=True)  # Reset index\n",
        "fndf = news_processed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Observations about dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fndf = news_processed\n",
        "unique_values = fndf['type'].unique()\n",
        "print(unique_values)\n",
        "#hard to know how to classify nan and unknown, so removed for now\n",
        "# we also remove the a weird type \n",
        "fndf = fndf.dropna(subset=['type'])\n",
        "fndf = fndf.loc[(fndf['type']!='unknown') & (fndf['type']!='unreliable') & (fndf['type'] != '2018-02-10 13:43:39.521661') & (fndf['type'] != \"rumor\")]\n",
        "# Need to reset index\n",
        "\n",
        "newunique_values = fndf['type'].unique()\n",
        "print(newunique_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# reliable, clickbait and political are all, by their contents, factually correct (albeit possibly politcally motivated)\n",
        "# we deem those to be \"real\" news\n",
        "print(\"adding binary labels\")\n",
        "fndf['type'] = fndf['type'].replace(r'^(reliable|clickbait|political)$', '0', regex=True) \n",
        "fndf['type'] = fndf['type'].replace(r'^(?!(0)$).+', '1', regex=True)   # Replace everything except '1' with '0'\n",
        "#maybe fix this? what remains?\n",
        "fndf['type'] = fndf['type'].fillna('0')\n",
        "\n",
        "newunique_values = fndf['type'].unique()\n",
        "print(newunique_values)\n",
        "fndf.shape[0]\n",
        "#fndf['type'] = fndf['type'].astype(int)  # Convert to integer\n",
        "\n",
        "print((\"real vs fake:\"))\n",
        "print(fndf['type'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "relib_news = fndf.loc[fndf['type'] == '0']\n",
        "fake_news = fndf.loc[fndf['type'] == '1']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "avg_len_real = relib_news[\"content\"].apply(len).mean()\n",
        "print(relib_news)\n",
        "avg_len_fake = fake_news[\"content\"].apply(len).mean()\n",
        "print(\"Average length of real news: \", avg_len_real)\n",
        "print(\"Average length of fake news: \", avg_len_fake)\n",
        "plt.bar([\"Real\", \"Fake\"], [avg_len_real, avg_len_fake])\n",
        "plt.title(\"Average length of news\")\n",
        "plt.ylabel(\"Average length\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fake_dist = FreqDist(fake_news['content'].explode())\n",
        "real_dist = FreqDist(relib_news['content'].explode())\n",
        "all_dist = FreqDist(fndf['content'].explode())\n",
        "# add one discounting\n",
        "fake_total = int(pd.Series([b+1 for (a, b) in fake_dist.most_common(10000)]).sum())\n",
        "real_total = int(pd.Series([b+1 for (a, b) in real_dist.most_common(10000)]).sum())\n",
        "all_total = int(pd.Series([b+1 for (a, b) in all_dist.most_common(10000)]).sum())\n",
        "print(f\"real total: {real_total}, fake total: {fake_total}\")\n",
        "print(real_total-fake_total)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "real_pd_prob = pd.Series(dict(real_dist)).apply(lambda x: x/real_total)\n",
        "fake_pd_prob = pd.Series(dict(fake_dist)).apply(lambda x: x/fake_total)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def entropy_calculation(row):\n",
        "    word = row[\"word\"]\n",
        "    # use one cause of add one discounting\n",
        "    row[\"real_prob\"] = real_dist.get(word, 1) / all_dist.get(word, 1/all_total)\n",
        "    #row[\"real_prob\"] = real_prob\n",
        "    row[\"fake_prob\"] = fake_dist.get(word, 1) / all_dist.get(word, 1/all_total)\n",
        "    #row[\"fake_prob\"] = fake_prob\n",
        "    #row[\"fake_prob\"] = -1*fake_prob*np.log2(fake_prob)\n",
        "    #row[\"real_entropy\"] = -1*real_prob*np.log2(real_prob)\n",
        "    #row[\"entropy\"] = row[\"fake_entropy\"] + row[\"real_entropy\"]\n",
        "    row[\"prob_diff\"] = abs(row[\"real_prob\"] - row[\"fake_prob\"])\n",
        "    return row\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "word_entropy = pd.DataFrame(all_dist.most_common(1000), columns=[\"word\", \"count\"]).apply(entropy_calculation, axis=1)\n",
        "word_entropy = word_entropy.sort_values(\"fake_prob\", ascending=False)\n",
        "print(word_entropy.head(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14, 9))\n",
        "\n",
        "word_entropy_lim = word_entropy.head(100)\n",
        "width = 0.2\n",
        "# Bar chart with stacking\n",
        "#plt.bar(word_entropy_lim[\"word\"], word_entropy_lim[\"real_prob\"] + word_entropy_lim[\"fake_prob\"], label=\"Real\", color=\"blue\")\n",
        "#plt.bar(word_entropy_lim[\"word\"], word_entropy_lim[\"fake_prob\"], label=\"Fake\", color=\"red\")\n",
        "\"\"\"\n",
        "plt.bar(word_entropy_lim[\"word\"])\n",
        "\n",
        "# Labels and title\n",
        "plt.xlabel(\"Words\")\n",
        "plt.ylabel(\"Entropy\")\n",
        "plt.title(\"Stacked Bar Chart of Real vs. Fake Entropy per Word\")\n",
        "plt.legend()\n",
        "\n",
        "# Rotate x-axis labels for readability\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n",
        "\"\"\"\n",
        "\n",
        "word_entropy_lim[[\"word\", \"real_prob\", \"fake_prob\"]].plot(kind='bar', x='word', stacked=False, figsize=(14, 9))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab = {val for (val, _) in all_dist.most_common(10000)}\n",
        "print(vocab)\n",
        "tfidf_real_vec = TfidfVectorizer(analyzer=lambda x: x, vocabulary=vocab)\n",
        "tfidf_fake_vec = TfidfVectorizer(analyzer=lambda x: x, vocabulary=vocab)\n",
        "tfidf_real = tfidf_real_vec.fit_transform(relib_news[\"content\"])\n",
        "tfidf_fake = tfidf_fake_vec.fit_transform(fake_news[\"content\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "tfidf_model = TfidfVectorizer(analyzer=lambda x: x, max_features=10000)\n",
        "tfidf_vec = tfidf_model.fit_transform(fndf[\"content\"])\n",
        "tfidf_vec_scale = StandardScaler(with_mean=False).fit_transform(tfidf_vec)\n",
        "PCA_model = PCA(n_components=2)\n",
        "PCA_data = PCA_model.fit_transform(tfidf_vec_scale)\n",
        "#PCA_data[\"type\"] = fndf[\"type\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "sns.scatterplot(x = PCA_data[:, 0], y = PCA_data[:, 1], hue=fndf[\"type\"], alpha=0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.kdeplot(x = PCA_data[:, 0], y = PCA_data[:, 1], hue=fndf[\"type\"], fill=False, alpha=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# observationer\n",
        "- det virker ikke til at de er seperatble kun udfra bag in a word med tf-idf\n",
        "- der er nogle regions hvor vi kan, og andre hvor vi ikke umiddelbart kan\n",
        "- måske er f.eks. embeddings bedre?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "real_pd = pd.Series(dict(real_dist))\n",
        "fake_pd = pd.Series(dict(fake_dist))\n",
        "\n",
        "real_set = set(real_pd.index)\n",
        "fake_set = set(fake_pd.index)\n",
        "unique_words = real_set.union(fake_set) - real_set.intersection(fake_set)\n",
        "print(f\"Amount of words only present in one set than another: {len(unique_words)}\")\n",
        "\n",
        "large_diff = [(word, real_pd.get(word, 0) - fake_pd.get(word, 0)) for word in fake_pd.index if abs(real_pd.get(word, 0) - fake_pd.get(word, 0)) > 0.00001]\n",
        "\n",
        "large_diff.sort(key=lambda x: abs(x[1]), reverse=True)\n",
        "print(large_diff)\n",
        "\n",
        "print(\"Unique tokens:\")\n",
        "for token in [\"url\", \"email\", \"num\", \"date\"]:\n",
        "    print(f\"TOKEN: {token} - Real: {real_pd[token]}, Fake: {fake_pd[token]}\")\n",
        "\n",
        "plt.bar(real_pd.keys(), real_pd.values)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEK4r7cy7doh"
      },
      "source": [
        "## Task 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxlKCVeK7doh",
        "outputId": "689ae9da-ca85-41ac-877b-dd07c1b5d147"
      },
      "outputs": [],
      "source": [
        "# Splitting into test, train and validation\n",
        "X_train_full, X_valtest_full, y_train, y_valtest = train_test_split(fndf, fndf['type'], test_size=0.2, random_state=42)\n",
        "X_test_full, X_val_full, y_test, y_val = train_test_split(X_valtest_full, y_valtest, test_size=0.5, random_state=42)\n",
        "# x_train = testing_ x, y_train = training_y\n",
        "# (x_test\n",
        "X_train = X_train_full['content']\n",
        "X_test = X_test_full['content']\n",
        "X_val = X_val_full['content']\n",
        "print(\"train size:\", y_train.shape)\n",
        "print(\"val size:\", y_val.shape)\n",
        "print(\"test size:\", y_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1MTfwNy7doh"
      },
      "source": [
        "# Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 0, splitting labels into reliable and unreliable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# done above as we used it for EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1 - Simple linear regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get the top 10000 words, and how often they occur in each article"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Standarize fndf?\n",
        "\n",
        "# get top 10000 words for vocab in training data to avoid leaking data from test set\n",
        "print(\"Finding vocabulary:\")\n",
        "vocab = X_train.explode().value_counts()[:10000].keys()\n",
        "\n",
        "vectorizer = CountVectorizer(analyzer=lambda x: x, vocabulary=vocab)\n",
        "print(\"vectorizing X_train\")\n",
        "rowsFreq = vectorizer.fit_transform(X_train)\n",
        "print(\"vectorizing X_val\")\n",
        "val_rowsFreq = vectorizer.fit_transform(X_test)\n",
        "\n",
        "print(\"vectorizing X_test\")\n",
        "test_rowsFreq = vectorizer.fit_transform(X_test)\n",
        "\n",
        "print(rowsFreq)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating the linear regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#scaler = StandardScaler(with_mean=False)\n",
        "scaler = Normalizer()\n",
        "undersampler = RandomUnderSampler(random_state=42)\n",
        "\n",
        "X_train_scale = scaler.fit_transform(rowsFreq)\n",
        "X_test_scale = scaler.transform(test_rowsFreq)\n",
        "X_val_scale = scaler.transform(val_rowsFreq)\n",
        "x_undersampled, y_undersampled = undersampler.fit_resample(X_train_scale, y_train)\n",
        "print(\"Starting model training:\")\n",
        "linReg = LogisticRegression(max_iter=1000, penalty=\"l1\", solver='liblinear', random_state=42)#, class_weight=\"balanced\")\n",
        "linReg_weighed = LogisticRegression(max_iter=1000, penalty=\"l1\", solver='liblinear', random_state=42, class_weight=\"balanced\")\n",
        "linReg_undersampled = LogisticRegression(max_iter=1000, penalty=\"l1\", solver='liblinear', random_state=42)\n",
        "print(\"fitting standard model\")\n",
        "linReg.fit(X_train_scale, y_train)\n",
        "print(\"fitting weighted model\")\n",
        "linReg_weighed.fit(X_train_scale, y_train)\n",
        "print(\"fitting undersampled model\")\n",
        "linReg_undersampled.fit(x_undersampled, y_undersampled)\n",
        "\n",
        "y_pred = linReg.predict(X_val_scale)\n",
        "y_pred_weighted = linReg_weighed.predict(X_val_scale)\n",
        "y_pred_undersampled = linReg_undersampled.predict(X_val_scale)\n",
        "f1 = f1_score(y_val, y_pred)\n",
        "f1_weighted = f1_score(y_val, y_pred_weighted)\n",
        "f1_undersampled = f1_score(y_val, y_pred_undersampled)\n",
        "# Print results\n",
        "print(\"Standard : Weighted : Undersampled\")\n",
        "print(f\"F1 Score: {f1:.4f} : {f1_weighted:.4f} : {f1_undersampled:.4f}\")\n",
        "print(f\"Hyperparameters: max_iter=1000, solver='liblinear', binary bag-of-words\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model\n",
        "joblib.dump((linReg, linReg_weighed, linReg_undersampled), dataPath + \"linReg.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Visualize\n",
        "val_rowsFreq = vectorizer.transform(X_val)\n",
        "y_val_pred = linReg_weighed.predict(scaler.transform(val_rowsFreq)).astype(int)\n",
        "print(f\"F1 Score: Eval: {f1_score(y_val, y_val_pred):.4f}, Test: {f1_score(y_val, y_val_pred):.4f}\")\n",
        "print(f\"Accuracy (eval set): {accuracy_score(y_val, y_val_pred):.4f}\")\n",
        "\n",
        "cm = confusion_matrix(y_val, y_val_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=linReg.classes_)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test set (created for evaluation in report)\n",
        "y_pred = linReg.predict(X_test_scale)\n",
        "y_train_pred = linReg.predict(X_train_scale)\n",
        "\n",
        "print(f\"F1 Score: Eval: {f1_score(y_train, y_train_pred):.4f}, Test: {f1_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Accuracy (test): {accuracy_score(y_test, y_pred_weighted):.4f}\")\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_weighted, normalize='true')\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=linReg.classes_)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bbc_raw = pd.read_csv(dataPath + \"bbc_articles.csv\")\n",
        "\n",
        "bbc_processed = processData(bbc_raw, 'text')\n",
        "bbc_processed['type'] = 1\n",
        "\n",
        "bbc_processed.to_json(dataPath + \"bbc_processed.json\", orient='records', lines=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load BBC df\n",
        "if bbc_proccessed is None:\n",
        "    json_reader = pd.read_json(dataPath + \"bbc_processed.json\", orient='records', lines=True, chunksize=1500)\n",
        "    bbc_processed = pd.concat(json_reader, ignore_index=True)\n",
        "\n",
        "bbcdf = bbc_processed\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(len(bbcdf['text']), len(bbcdf['type']))\n",
        "#print(len(BBC_train), len(BBC_y))  # These should be the same\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Load bbcReg\n",
        "bbcReg = joblib.load(dataPath + \"bbcReg.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BBC_train = pd.concat([X_train, bbcdf['text']]).reset_index(drop=True)\n",
        "BBC_y = pd.concat([y_train, bbcdf[\"type\"]])\n",
        "\n",
        "BBC_vocab = BBC_train.explode().value_counts()[:10000].keys()\n",
        "vectorizer = CountVectorizer(analyzer=lambda x: x, vocabulary=BBC_vocab)\n",
        "bbcFreq = vectorizer.fit_transform(BBC_train)\n",
        "bbc_testFreq = vectorizer.fit_transform(X_test)\n",
        "\n",
        "scaler = Normalizer()\n",
        "BBC_train = scaler.fit_transform(bbcFreq)\n",
        "BBC_test = scaler.fit_transform(bbc_testFreq)\n",
        "bbcReg = LogisticRegression(max_iter=1000, penalty=\"l1\", solver='liblinear', random_state=42)\n",
        "bbcReg.fit(BBC_train, BBC_y)\n",
        "BBC_pred = bbcReg.predict(bbc_testFreq)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Printing results\n",
        "print(f\"Accuracy (test): {accuracy_score(y_test, BBC_pred):.4f}\")\n",
        "print(f\"F1: {f1_score(y_test, BBC_pred): .4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "joblib.dump(bbcReg, dataPath + \"bbcReg.pkl\")\n",
        "joblib.dump((bbcFreq, bbc_testFreq), dataPath + \"bbcFreqs.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# advanced model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(X_train.to_frame().reset_index(drop=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "print(\"tagging documents\")\n",
        "tagged_docs = [TaggedDocument(words=doc, tags=[i]) for doc, i in zip(X_train.values, range(X_train.shape[0]))]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vector_size = 100\n",
        "embed_model = gensim.models.doc2vec.Doc2Vec(tagged_docs, vector_size=vector_size, min_count=2, workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embed_model.save(dataPath + \"doc2vec.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embed_model = gensim.models.doc2vec.Doc2Vec.load(dataPath + \"doc2vec.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_embedded = pd.Series([embed_model.infer_vector(doc) for doc in X_train])\n",
        "X_val_embed = pd.Series([embed_model.infer_vector(doc) for doc in X_val])\n",
        "X_test_embed = pd.Series([embed_model.infer_vector(doc) for doc in X_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "joblib.dump((X_train_embedded, X_val_embed, X_test_embed), dataPath + \"X_test_embed.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(X_train_embedded, X_val_embed, X_test_embed) = joblib.load(dataPath + \"X_test_embed.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "PCA_embedded = PCA(n_components=2)\n",
        "trained_embedded = pd.DataFrame(embed_model.dv.vectors)\n",
        "PCA_embedded.fit(trained_embedded)\n",
        "val_embedded = PCA_embedded.transform(X_val_embed.to_list())\n",
        "val_embedded = pd.DataFrame(val_embedded, columns=[\"PCA1\", \"PCA2\"])\n",
        "val_embedded[\"type\"] = y_test\n",
        "\n",
        "sns.kdeplot(data=val_embedded, x=\"PCA1\", y=\"PCA2\", hue=\"type\", alpha=0.9)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combination of various models attempted on doc embeddings\n",
        "\"\"\"\n",
        "svc = LinearSVC(random_state=42, verbose=1, class_weight=\"balanced\")\n",
        "svc.fit(X_train_embedded, y_train)\n",
        "y_pred_svc = svc.predict(X_test_embed.to_list())\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred_svc):.4f}\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_svc):.4f}\")\n",
        "\"\"\"\n",
        "scaler = Normalizer()\n",
        "tf_idf_model = TfidfVectorizer(max_features=10000, analyzer=lambda x: x)\n",
        "tfidf_train = tf_idf_model.fit_transform(X_train)\n",
        "tfidf_train = scaler.fit_transform(tfidf_train)\n",
        "# alpha = 0.001 = 0.88 f1\n",
        "mlp = MLPClassifier(max_iter = 2000, random_state=42, verbose=1, early_stopping=True, alpha=0.0015)\n",
        "mlp.fit(tfidf_train, y_train)\n",
        "tfidf_test = tf_idf_model.transform(X_test)\n",
        "tfidf_test = scaler.transform(tfidf_test)\n",
        "y_pred_mlp = mlp.predict(tfidf_test)\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred_mlp):.4f}\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_mlp):.4f}\")\n",
        "\n",
        "#building neural network\n",
        "# 0.777 both acc and f1\n",
        "\"\"\"\n",
        "mlp = MLPClassifier(max_iter = 2000, random_state=42, verbose=1)\n",
        "mlp.fit(X_train_embedded, y_train)\n",
        "y_pred_mlp = mlp.predict(X_test_embed.to_list())\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred_mlp):.4f}\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_mlp):.4f}\")\n",
        "# f1 =. .8048\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# creating TF-IDF vectors for advanced voting classifier\n",
        "scaler = Normalizer()\n",
        "tf_idf_model = TfidfVectorizer(max_features=10000, analyzer=lambda x: x)\n",
        "tfidf_train = tf_idf_model.fit_transform(X_train)\n",
        "tfidf_train = scaler.fit_transform(tfidf_train)\n",
        "\n",
        "tfidf_test = tf_idf_model.transform(X_test)\n",
        "tfidf_test = scaler.transform(tfidf_test)\n",
        "\n",
        "tfidf_val = tf_idf_model.transform(X_val)\n",
        "tfidf_val = scaler.transform(tfidf_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import scipy.sparse as sp\n",
        "\n",
        "# scale doc embeddings\n",
        "scaler = Normalizer()\n",
        "X_train_embedded_scaled = scaler.fit_transform(X_train_embedded)\n",
        "X_test_embed_scaled = scaler.transform(X_test_embed)\n",
        "\n",
        "# double feature set\n",
        "print(tfidf_train.shape)\n",
        "print(X_train_embedded.shape)\n",
        "print(X_test_embed.shape)\n",
        "print(type(tfidf_train))\n",
        "print(type(X_train_embedded))\n",
        "print(type(X_test_embed))\n",
        "dense_train_embed = sp.csr_matrix(X_train_embedded_scaled)\n",
        "dense_test_embed = sp.csr_matrix(X_test_embed_scaled)\n",
        "total_train = sp.hstack([tfidf_train, dense_train_embed])\n",
        "total_test = sp.hstack([tfidf_test, dense_test_embed])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_val_embed_scale = scaler.transform(X_val_embed)\n",
        "dense_val_embed = sp.csr_matrix(X_val_embed_scale)\n",
        "total_val = sp.hstack([tfidf_val, dense_val_embed])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# creating voting classifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "mlp_total = MLPClassifier(max_iter = 2000, random_state=42, verbose=1, alpha=0.01, early_stopping=True)\n",
        "\n",
        "neural_only_embed = Pipeline([\n",
        "    ('only_embed', ColumnTransformer([('embeds', 'passthrough', slice(10000, None))])),\n",
        "    ('scaler', Normalizer()),\n",
        "    ('mlp', MLPClassifier(max_iter = 2000, random_state=42, verbose=1))\n",
        "])\n",
        "\n",
        "neural_only_tfidf = Pipeline([\n",
        "    ('only_tfidf', ColumnTransformer([('tfidf', 'passthrough', slice(0, 10000))])),\n",
        "    ('mlp', MLPClassifier(max_iter = 2000, random_state=42, verbose=1, early_stopping=True, alpha=0.005))\n",
        "])\n",
        "\n",
        "\n",
        "voting = VotingClassifier(estimators=[('only_tfidf', neural_only_tfidf), ('mlp', mlp_total), ('only_embed', neural_only_embed)], voting='soft', weights=[0.75, 1, 0.75])\n",
        "voting.fit(total_train, y_train)\n",
        "y_pred = voting.predict(total_test)\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "joblib.dump(voting, dataPath + \"voting_weighted.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "voting = joblib.load(dataPath + \"voting_weighted.pkl\")\n",
        "y_val = y_val.astype(int)\n",
        "y_test = y_test.astype(int)\n",
        "\n",
        "y_pred_total = voting.predict(total_val)\n",
        "print(f\"Accuracy (test): {accuracy_score(y_val, y_pred_total):.4f}\")\n",
        "print(\"F1-score: \", f1_score(y_val, y_pred_total))\n",
        "\n",
        "print(\"Recall: \", recall_score(y_val, y_pred_total))\n",
        "print(\"Precision: \", precision_score(y_val, y_pred_total))\n",
        "\n",
        "cm = confusion_matrix(y_val, y_pred_total, normalize='true')\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=voting.classes_)\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "cm = confusion_matrix(y_val, y_pred_total)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=voting.classes_)\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "for model in voting.estimators_:\n",
        "    print(f\"F1 Score: {f1_score(y_val, model.predict(total_val)):.4f}\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_val, model.predict(total_val)):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# eval for evaluation - using test set\n",
        "y_pred_total = voting.predict(total_test)\n",
        "print(f\"Accuracy (test): {accuracy_score(y_test, y_pred_total):.4f}\")\n",
        "print(f\"f1_score: {f1_score(y_test, y_pred_total):.4f}\")\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_total, normalize='true')\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=voting.classes_)\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "for model in voting.estimators_:\n",
        "    print(f\"F1 Score: {f1_score(y_test, model.predict(total_test)):.4f}\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, model.predict(total_test)):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Trying HistGradientBoostingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=10000, analyzer=lambda x: x)\n",
        "X_training = vectorizer.fit_transform(X_train)\n",
        "X_testing = vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Gradient Descent random forest\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=400)\n",
        "\n",
        "X_training_pca = pca.fit_transform(X_training)\n",
        "X_testing_pca = pca.transform(X_testing)\n",
        "\n",
        "\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "\n",
        "HGBC = HistGradientBoostingClassifier(max_iter=500, random_state=42, verbose=1)\n",
        "\n",
        "HGBC.fit(X_training_pca, y_train)\n",
        "y_pred = HGBC.predict(X_testing_pca)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# k nearest neighbour\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import Normalizer\n",
        "scaler = Normalizer()\n",
        "X_test_embed = np.array([np.array(x) for x in X_test_embed])\n",
        "print(X_test_embed)\n",
        "print(X_train_embedded.shape)\n",
        "\n",
        "X_train_embed_scale = scaler.fit_transform(X_train_embedded)\n",
        "X_test_embed_scale = scaler.fit_transform(X_test_embed)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
        "print(\"fitting model\")\n",
        "knn.fit(X_train_embed_scale, y_train)\n",
        "y_pred_knn = knn.predict(X_test_embed_scale)\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred_knn):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "13df6b96a88944e98d65e813eeb54bd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2814e4c7aa32487e93e3ccd90b6e5f0c",
              "IPY_MODEL_3cc027a216d2481ab36faca98212f3c5"
            ],
            "layout": "IPY_MODEL_4ae5f28587fe4d01ad03446e234a631d"
          }
        },
        "1fdcaf3f9a5245eea4a903b928eee5b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b58df1204e824dbd9f72cdb606b2605f",
              "IPY_MODEL_b1c6087d3824401aa91057fe441302fe"
            ],
            "layout": "IPY_MODEL_830cda2af3cb49ecbfe45ca7f2e04a24"
          }
        },
        "23088918d37a416a8506b3fd4c7231f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "24f8c63d4dc3428fbcd67efc20c076f2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2814e4c7aa32487e93e3ccd90b6e5f0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "IntProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100.00%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e28976f80c1548f2af7836d8e8491c87",
            "max": 232,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_23088918d37a416a8506b3fd4c7231f9",
            "value": 232
          }
        },
        "3cc027a216d2481ab36faca98212f3c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24f8c63d4dc3428fbcd67efc20c076f2",
            "placeholder": "​",
            "style": "IPY_MODEL_6cfbf875045943ad8b40671a67e0e29e",
            "value": "232 / 232"
          }
        },
        "4ae5f28587fe4d01ad03446e234a631d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cfbf875045943ad8b40671a67e0e29e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e08915023cb458abda3abdce421467e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7258d6e2da3a41ad9b4830b0192cb037": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "830cda2af3cb49ecbfe45ca7f2e04a24": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fd9741bc796445dacef77f07614167e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1fdcaf3f9a5245eea4a903b928eee5b8"
            ],
            "layout": "IPY_MODEL_e347b4d8b9fb4c72929f6a646167747c"
          }
        },
        "a24a494c9ffb43e1bfe493d1162c4e3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1c6087d3824401aa91057fe441302fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9249597abf942b5af7a16e2ebefa329",
            "placeholder": "​",
            "style": "IPY_MODEL_a24a494c9ffb43e1bfe493d1162c4e3d",
            "value": "0 / 812913"
          }
        },
        "b58df1204e824dbd9f72cdb606b2605f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "IntProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "0.00%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e08915023cb458abda3abdce421467e",
            "max": 812913,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7258d6e2da3a41ad9b4830b0192cb037",
            "value": 0
          }
        },
        "bf68c788a3d9437383fdf54f8f75f050": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_13df6b96a88944e98d65e813eeb54bd4"
            ],
            "layout": "IPY_MODEL_dc1dd2e88478465e8ebc8082b1aa84aa"
          }
        },
        "d9249597abf942b5af7a16e2ebefa329": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc1dd2e88478465e8ebc8082b1aa84aa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e28976f80c1548f2af7836d8e8491c87": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e347b4d8b9fb4c72929f6a646167747c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
