{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/andreas-linus-thalund-\n",
      "[nltk_data]     midtgaard/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/andreas-linus-\n",
      "[nltk_data]     thalund-midtgaard/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.lm import Vocabulary\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from cleantext import clean\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords') \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 250 entries, 0 to 249\n",
      "Data columns (total 16 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Unnamed: 0        250 non-null    int64  \n",
      " 1   id                250 non-null    int64  \n",
      " 2   domain            250 non-null    object \n",
      " 3   type              238 non-null    object \n",
      " 4   url               250 non-null    object \n",
      " 5   content           250 non-null    object \n",
      " 6   scraped_at        250 non-null    object \n",
      " 7   inserted_at       250 non-null    object \n",
      " 8   updated_at        250 non-null    object \n",
      " 9   title             250 non-null    object \n",
      " 10  authors           170 non-null    object \n",
      " 11  keywords          0 non-null      float64\n",
      " 12  meta_keywords     250 non-null    object \n",
      " 13  meta_description  54 non-null     object \n",
      " 14  tags              27 non-null     object \n",
      " 15  summary           0 non-null      float64\n",
      "dtypes: float64(2), int64(2), object(12)\n",
      "memory usage: 31.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "dataPath = \"../data/\"\n",
    "newsSample = pd.read_csv(dataPath + \"news_sample.csv\")\n",
    "nsdf = pd.DataFrame(newsSample)\n",
    "nsdf = nsdf.reset_index(drop=True)  # Reset index??\n",
    "print(nsdf.info())   # Check column types and missing values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unreliable' 'fake' 'clickbait' 'conspiracy' 'reliable' 'bias' 'hate'\n",
      " 'junksci' 'political' nan 'unknown']\n"
     ]
    }
   ],
   "source": [
    "# unique lable values\n",
    "unique_values = nsdf['type'].unique()\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unreliable' 'fake' 'clickbait' 'conspiracy' 'reliable' 'bias' 'hate'\n",
      " 'junksci' 'political']\n"
     ]
    }
   ],
   "source": [
    "#nan and unknown removed as they seem useless when training a classifier\n",
    "nsdf = nsdf.dropna(subset=['type'])\n",
    "nsdf = nsdf.loc[nsdf['type']!='unknown']\n",
    "newunique_values = nsdf['type'].unique()\n",
    "print(newunique_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_help(text):\n",
    "    if isinstance(text, str):\n",
    "        return clean(text, lower=True, replace_with_url=\"<URL>\", replace_with_email=\"<EMAIL>\", replace_with_number=\"<NUMBER>\")\n",
    "    return text  # Return unchanged if not a string\n",
    "\n",
    "def cleanText(data, column):\n",
    "    data = data.reset_index(drop=True)  # Reset index\n",
    "    data[column] = data[column].apply(clean_text_help)  # Apply function\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsdf_cleaned = cleanText(nsdf, 'content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the text function\n",
    "def tokenize_text_help(text):\n",
    "    if isinstance(text, str):\n",
    "        return word_tokenize(text)\n",
    "    return text  # Return unchanged if not a string\n",
    "\n",
    "def tokenizeText(data, column):\n",
    "    data[column] = data[column].apply(tokenize_text_help)  # Apply function\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for removeing stopwords\n",
    "def remove_stopwords_help(text):\n",
    "    stop_words = set(stopwords.words('english'))  # Load stopwords\n",
    "    if isinstance(text, str):\n",
    "        return [word for word in text.at[0, 'content'] if not word.lower() in stop_words]\n",
    "    return text  # Return unchanged if not a string\n",
    "\n",
    "def remove_stopwords(data, column):\n",
    "    data[column] = data[column].apply(remove_stopwords_help)  # Apply function\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funtion for populating vocabulary\n",
    "def populate_vocabulary(data):\n",
    "    N = data.shape[0]  # Get the number of rows\n",
    "    allWords = []\n",
    "    for i in range(N):\n",
    "        if isinstance(data.at[i, 'content'], str):  # Ensure it's a string\n",
    "            allWords.append(data.at[i, 'content'])\n",
    "    return Vocabulary(allWords, unk_cutoff=2)\n",
    "\n",
    "#langsom k√∏rertid men kunne ikke finde ud af det med apply. Nogne med en god ide??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sometimes', 'the', 'power', 'of', 'christmas', 'will', 'make', 'you', 'do', 'wild', 'and', 'wonderful', 'things', '.', 'you', 'do', 'not', 'need', 'to', 'believe', 'in', 'the', 'holy', 'trinity', 'to', 'believe', 'in', 'the', 'positive', 'power', 'of', 'doing', 'good', 'for', 'others', '.', 'the', 'simple', 'act', 'of', 'giving', 'without', 'receiving', 'is', 'lost', 'on', 'many', 'of', 'us', 'these', 'days', ',', 'as', 'worries', 'about', 'money', 'and', 'success', 'hold', 'us', 'back', 'from', 'giving', 'to', 'others', 'who', 'are', 'in', 'need', '.', 'one', 'congregation', 'in', 'ohio', 'was', 'moved', 'to', 'action', 'by', 'the', 'power', 'of', 'a', 'sermon', 'given', 'at', 'their', 'church', 'on', 'christmas', 'eve', '.', 'the', 'pastor', 'at', 'grand', 'lake', 'united', 'methodist', 'church', 'in', 'celina', ',', 'ohio', 'gave', 'an', 'emotional', 'sermon', 'about', 'the', 'importance', 'of', 'understanding', 'the', 'message', 'of', 'jesus', '.', 'for', 'many', 'religious', 'people', 'the', 'message', 'of', 'jesus', 'is', 'to', 'help', 'others', 'before', 'yourself', ',', 'to', 'make', 'sure', 'the', 'people', 'who', 'are', 'suffering', 'get', 'the', 'help', 'they', 'need', 'to', 'enjoy', 'life', 'a', 'little', 'bit', '.', 'the', 'sermon', 'was', 'really', 'about', 'generosity', 'and', 'what', 'that', 'can', 'look', 'like', 'in', 'our', 'lives', '.', 'jesus', 'lived', 'a', 'long', 'time', 'ago', 'and', 'he', 'acted', 'generously', 'in', 'the', 'fashion', 'of', 'his', 'time', 'but', 'what', 'would', 'a', 'generous', 'act', 'look', 'like', 'in', 'our', 'times', '?', 'that', 'was', 'the', 'focus', 'of', 'the', 'sermon', '.', 'the', 'potency', 'of', 'the', 'sermon', 'was', 'not', 'lost', 'on', 'the', 'congregation', ',', 'who', 'were', 'so', 'moved', 'they', 'had', 'to', 'take', 'action', '!', 'after', 'the', 'sermon', 'ended', ',', 'the', 'congregation', 'decided', 'to', 'take', 'an', 'offering', '.', 'a', 'bowl', 'was', 'passed', 'around', 'the', 'room', 'and', 'everyone', 'pitched', 'in', 'what', 'they', 'could', 'on', 'this', 'christmas', 'eve', 'with', 'the', 'words', 'of', 'the', 'sermon', 'still', 'ringing', 'in', 'their', 'ears', '.', 'what', 'did', 'they', 'do', 'with', 'this', 'offering', '?', 'members', 'of', 'the', 'congregation', 'drove', 'down', 'to', 'the', 'local', 'waffle', 'house', 'to', 'visit', 'the', 'ladies', 'working', 'the', 'night', 'shift', '.', 'what', 'a', 'great', 'choice', 'on', 'this', 'most', 'holy', 'of', 'days', 'when', 'everyone', 'should', 'be', 'with', 'their', 'families', '!', 'the', 'ladies', 'working', 'at', 'waffle', 'house', 'clearly', 'were', 'not', 'with', 'their', 'families', '.', 'they', 'had', 'no', 'choice', 'but', 'to', 'work', 'on', 'this', 'holy', 'day', 'because', 'it', 'paid', 'the', 'bills', '.', 'the', 'congregation', 'understood', 'the', 'sacrifice', 'being', 'made', 'by', 'these', 'ladies', ',', 'and', 'wanted', 'to', 'help', 'them', 'out', '.', 'they', 'donated', 'the', 'entire', 'offering', 'to', 'be', 'split', 'amongst', 'the', 'ladies', 'at', 'waffle', 'house', '.', 'in', 'total', 'that', 'amounted', 'to', '$', '3,500', 'being', 'split', 'amongst', 'the', 'staff', '.', 'what', 'a', 'beautiful', 'moment', '!', 'what', 'a', 'perfect', 'example', 'of', 'what', 'the', 'preacher', 'was', 'talking', 'about', 'in', 'his', 'sermon', '!', 'doing', 'a', 'good', 'deed', 'like', 'this', 'on', 'christmas', 'really', 'helped', 'ease', 'the', 'burden', 'felt', 'by', 'the', 'ladies', 'working', 'at', 'waffle', 'house', '.', 'sure', ',', 'they', 'could', 'not', 'see', 'their', 'families', ',', 'but', 'at', 'least', 'they', 'got', 'a', 'little', 'gift', 'from', 'the', 'good', 'people', 'of', 'their', 'community', '.', 'perhaps', 'the', 'best', 'part', 'about', 'this', 'whole', 'event', 'was', 'that', 'the', 'congregation', 'did', 'not', 'ask', 'anything', 'in', 'return', '.', 'it', 'was', 'a', 'simple', 'act', 'of', 'generosity', 'from', 'people', 'who', 'understood', 'the', 'pain', 'being', 'felt', 'by', 'another', 'group', 'and', 'sought', 'to', 'alleviate', 'some', 'of', 'that', 'pain', '.', 'it', 'speaks', 'volumes', 'about', 'the', 'merits', 'of', 'the', 'church', 'in', 'our', 'daily', 'lives', '.', 'this', 'simple', 'act', 'brought', 'the', 'entire', 'community', 'together', 'because', 'it', 'showed', 'empathy', 'and', 'compassion', 'on', 'the', 'most', 'special', 'day', 'of', 'the', 'year', '.']\n"
     ]
    }
   ],
   "source": [
    "nsdf_tokenized = tokenizeText(nsdf_cleaned, 'content')                  #tokenizing\n",
    "nsdf_preprocessed = remove_stopwords(nsdf_tokenized, 'content')           #removing stopwords\n",
    "print(nsdf_preprocessed.at[0, 'content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#size of vocabulary\n",
    "vocabulary = populate_vocabulary(nsdf_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "\n",
    "fakeNewsCorpus = pd.read_csv(dataPath + \"995,000_rows.csv\")\n",
    "#Hva saten er den der unnamed???\n",
    "print(fakeNewsCorpus.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 995000 entries, 0 to 994999\n",
      "Data columns (total 17 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   Unnamed: 0        994999 non-null  object \n",
      " 1   id                994993 non-null  object \n",
      " 2   domain            994989 non-null  object \n",
      " 3   type              947214 non-null  object \n",
      " 4   url               994989 non-null  object \n",
      " 5   content           994988 non-null  object \n",
      " 6   scraped_at        994987 non-null  object \n",
      " 7   inserted_at       994987 non-null  object \n",
      " 8   updated_at        994987 non-null  object \n",
      " 9   title             986394 non-null  object \n",
      " 10  authors           552243 non-null  object \n",
      " 11  keywords          0 non-null       float64\n",
      " 12  meta_keywords     956210 non-null  object \n",
      " 13  meta_description  469894 non-null  object \n",
      " 14  tags              230919 non-null  object \n",
      " 15  summary           0 non-null       float64\n",
      " 16  source            214922 non-null  object \n",
      "dtypes: float64(2), object(15)\n",
      "memory usage: 129.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(fakeNewsCorpus.info())   # Check column types and missing values\n",
    "fndf = pd.DataFrame(fakeNewsCorpus)\n",
    "fndf = fndf.reset_index(drop=True)  # Reset index??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                                                        732\n",
       "id                                                          7444726.0\n",
       "domain                                             nationalreview.com\n",
       "type                                                        political\n",
       "url                 http://www.nationalreview.com/node/152734/%E2%...\n",
       "content             Plus one article on Google Plus\\n\\n(Thanks to ...\n",
       "scraped_at                                 2017-11-27T01:14:42.983556\n",
       "inserted_at                                2018-02-08 19:18:34.468038\n",
       "updated_at                                 2018-02-08 19:18:34.468066\n",
       "title                                              Iran News Round Up\n",
       "authors                                                           NaN\n",
       "keywords                                                          NaN\n",
       "meta_keywords       ['National Review', 'National Review Online', ...\n",
       "meta_description                                                  NaN\n",
       "tags                                                              NaN\n",
       "summary                                                           NaN\n",
       "source                                                            NaN\n",
       "Name: 0, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Pandas DataFrame:\")\n",
    "display(fndf.iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['political' 'fake' 'satire' 'reliable' 'conspiracy' 'unreliable' 'bias'\n",
      " 'rumor' 'unknown' nan 'clickbait' 'hate' 'junksci'\n",
      " '2018-02-10 13:43:39.521661']\n"
     ]
    }
   ],
   "source": [
    "# unique lable values\n",
    "unique_values = fndf['type'].unique()\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['political' 'fake' 'satire' 'reliable' 'conspiracy' 'unreliable' 'bias'\n",
      " 'rumor' 'clickbait' 'hate' 'junksci' '2018-02-10 13:43:39.521661']\n"
     ]
    }
   ],
   "source": [
    "#hard to know how to classify nan and unknown, so removed for now\n",
    "fndf = fndf.dropna(subset=['type'])\n",
    "fndf = fndf.loc[fndf['type']!='unknown']\n",
    "\n",
    "newunique_values = fndf['type'].unique()\n",
    "print(newunique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '1']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "903680"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groups (reliable) as truenews 1 and (all others) in fakenews 0 \n",
    "    #note this is naive and should be reconsidered later\n",
    "fndf['type'] = fndf['type'].replace(r'^reliable$', '1', regex=True)  # Only replaces exact 'reliable' with 1\n",
    "fndf['type'] = fndf['type'].replace(r'^(?!1$).+', '0', regex=True)   # Replace everything except '1' with '0'\n",
    "#fndf['type'] = fndf['type'].fillna('0')\n",
    "\n",
    "newunique_values = fndf['type'].unique()\n",
    "print(newunique_values)\n",
    "fndf.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: (722944,)\n",
      "val size: (90368,)\n",
      "test size: (90368,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting into test, train and validation\n",
    "X_train, X_valtest, y_train, y_valtest = train_test_split(fndf['content'], fndf['type'], test_size=0.2, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_valtest, y_valtest, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"train size:\", y_train.shape)\n",
    "print(\"val size:\", y_val.shape)\n",
    "print(\"test size:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
